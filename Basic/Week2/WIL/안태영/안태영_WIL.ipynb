{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 06: Softmax Regression**"
      ],
      "metadata": {
        "id": "QFT7ZErnTSdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Multinomial Classification**  \n",
        "- Binary classfication 모델을 여러 개 만들어서 여러가지 군으로 분리하는 것\n",
        "- 2차원 행렬을 연산하여 진행한다\n",
        "- ex) 3개의 label을 분류한다 했을 때는 $w_{11}x_{1} + w_{12}x_{2} + w_{13}x_{3}, w_{21}x_{1} + w_{22}x_{2} + w_{23}x_{3}, w_{31}x_{1} + w_{32}x_{2} + w_{33}x_{3} \\cdots$ 이런식으로 나열해야 한다 하지만 행렬 연산을 진행 했을땐,  $\\left[\\begin{matrix} w_{11} w_{12} w_{13}\\\\ w_{21} w_{22} w_{23}\\\\ w_{31} w_{32} w_{33}\\\\ \\end{matrix}\\right] \\cdot \\left[\\begin{matrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ \\end{matrix}\\right]$ 로 표현 가능함\n",
        "\n",
        "> **Softmax**  \n",
        "- 어떤 입력 값에 대해서 각각의 원소에 대한 확률을 나타내주는 형태의 sigmoid 대체 함수\n",
        "- 값들은 0과 1사이이다\n",
        "- 각각의 원소의 합들이 1로 나타내어 진다  \n",
        "- $S(y_{i}) = \\frac{e^{y_{i}}}{\\sum{e^{y_{j}}}}$  \n",
        "\n",
        "> **One-Hot Encoding**  \n",
        "- Softmax 함수를 거쳐서 나온 값중 가장 큰 값의 확률을 1로 바꾸는 과정  \n",
        "- tensorflow에서는 argmax라는 함수가 담당한다  \n",
        "\n",
        "\n",
        "> **Cross Entropy**  \n",
        "- Multi label classification에서 cost\n",
        "- -$\\sum\\limits_{i} L_{i}\\log{S_{i}} = \\sum\\limits_{i} L_{i} * (-\\log{S_{i}})$  \n",
        "\n",
        "> **Cost Function**  \n",
        "- $Loss = \\frac{1}{N} \\sum\\limits_{i} D({S(wx_{i} + b), L_{i}})$  \n",
        "- 여기서 $S(wx_{i} + b)$는 y 값, $L_{i}$는 확률 값이다  \n",
        "\n",
        "> **Gradient Descent**  \n",
        "- 이번엔 각각의 weight 벡터에 대한 gradient의 편미분을 말하는 것이다  \n"
      ],
      "metadata": {
        "id": "EZVOM4K0VphA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 06-1: Softmax Classification Eager**"
      ],
      "metadata": {
        "id": "jNNBPwIChTZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfCZYQCSSztc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(777)  # for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Data를 벡터 형태로 담는다**  \n",
        "**nb_class**  \n",
        "&rarr; nb_class는 몇개의 sector로 분류할 것인지에 대한 변수로, 행의 개수를 정의하는 것이다\n"
      ],
      "metadata": {
        "id": "Mo0H1355hnSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data\n",
        "x_data = [[1, 2, 1, 1],\n",
        "          [2, 1, 3, 2],\n",
        "          [3, 1, 3, 4],\n",
        "          [4, 1, 5, 5],\n",
        "          [1, 7, 5, 5],\n",
        "          [1, 2, 5, 6],\n",
        "          [1, 6, 6, 6],\n",
        "          [1, 7, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "#convert into numpy and float format\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)\n",
        "\n",
        "#nb_classes\n",
        "nb_classes = 3\n",
        "print(x_data.shape)\n",
        "print(y_data.shape)"
      ],
      "metadata": {
        "id": "S1cSLuN6elVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Hyopothesis에 들어갈 weight값과 bias 설정**"
      ],
      "metadata": {
        "id": "boZ9qPAAimrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
        "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
        "variables = [W, b]\n",
        "\n",
        "print(W,b)"
      ],
      "metadata": {
        "id": "erO8GRt9ioDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Hypothesis**"
      ],
      "metadata": {
        "id": "r_9K6w3wicAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesis(X):\n",
        "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "print(hypothesis(x_data))"
      ],
      "metadata": {
        "id": "KY5enhLb6AY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Softmax funciton**"
      ],
      "metadata": {
        "id": "5xWlhlQxexaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_db = [[8,2,1,4]]\n",
        "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
        "\n",
        "\n",
        "print(hypothesis(sample_db))"
      ],
      "metadata": {
        "id": "Rpp8Njpeerl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Cost function**"
      ],
      "metadata": {
        "id": "c_5u9gVg6OF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_fn(X, Y):\n",
        "    logits = hypothesis(X)\n",
        "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
        "    cost_mean = tf.reduce_mean(cost)\n",
        "\n",
        "    return cost_mean\n",
        "\n",
        "print(cost_fn(x_data, y_data))"
      ],
      "metadata": {
        "id": "mAKi4Yo6g896"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Gradient Tapee**"
      ],
      "metadata": {
        "id": "MA1Ku5Ye6l_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = x * x # x^2\n",
        "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
        "print(dy_dx)"
      ],
      "metadata": {
        "id": "mcKgAWsy6Sk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Gradient**"
      ],
      "metadata": {
        "id": "blhGaaNP6qjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_fn(X, Y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = cost_fn(X, Y)\n",
        "        grads = tape.gradient(loss, variables)\n",
        "\n",
        "        return grads\n",
        "\n",
        "print(grad_fn(x_data, y_data))"
      ],
      "metadata": {
        "id": "OodPvFOY6ovL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Model fitting**"
      ],
      "metadata": {
        "id": "uo8nGSvk6w1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, Y, epochs=2000, verbose=100):\n",
        "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        grads = grad_fn(X, Y)\n",
        "        optimizer.apply_gradients(zip(grads, variables))\n",
        "        if (i==0) | ((i+1)%verbose==0):\n",
        "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
        "\n",
        "fit(x_data, y_data)"
      ],
      "metadata": {
        "id": "PmfD4YyJ6tpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Argmax를 이용한 정확도 측정**"
      ],
      "metadata": {
        "id": "Jzscugig6_sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]]\n",
        "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
        "\n",
        "a = hypothesis(sample_data)\n",
        "\n",
        "print(a)\n",
        "print(tf.argmax(a, 1)) #index: 2\n",
        "\n",
        "b = hypothesis(x_data)\n",
        "print(b)\n",
        "print(tf.argmax(b, 1))\n",
        "print(tf.argmax(y_data, 1)) # matches with y_data"
      ],
      "metadata": {
        "id": "K3QrZ0Ln618K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 06-2: Softmax Zoo Classifier-Eager**"
      ],
      "metadata": {
        "id": "yNjbzQWM7_Aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **tf.onehot()**  \n",
        "&rarr; 내가 원하는 행의 개수 만큼 행렬을 변환해주는 method  \n",
        "&rarr; 3차원으로 반환  \n",
        "**tf.reshape()**  \n",
        "&rarr; 원하는 형태의 행렬로 재배열 해준다  \n",
        "&rarr; 앞서 3차원으로 반환 되었지만 2차원 형태로 model fitting을 해야하기 때문에 reshape를 사용한다\n"
      ],
      "metadata": {
        "id": "9DpA5SJt9JqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, -1]\n",
        "\n",
        "nb_classes = 7  # 0 ~ 6\n",
        "\n",
        "# Make Y data as onehot shape\n",
        "#2차원에서 3차원으로 변환\n",
        "#tf.one_hot()을 쓰면 3차원으로 반환을 해주기 때문에\n",
        "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
        "print(x_data.shape, Y_one_hot.shape)"
      ],
      "metadata": {
        "id": "kgH-iJM07JbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Weight와 Bias**   \n",
        "&rarr; 전과 같음  \n",
        "**Logit Function, Hypothesis**  \n",
        "&rarr; 나중에 정확도 관련 값을 구할때 필요하기 때문에 따로 정의한다  \n",
        "**Cross Entropy**  \n",
        "&rarr; tf.keras.losses.categorical_crossentropy() 이용  \n",
        "**tf.argmax()**  \n",
        "&rarr; 가장 큰값을 가지는 index를 리턴해준다  \n",
        "**Prediction**  \n",
        "&rarr; Accuracy를 알려주는 함수  "
      ],
      "metadata": {
        "id": "w9E523FU-om4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Weight and bias setting\n",
        "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n",
        "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
        "variables = [W, b]\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "\n",
        "#####logit과 hypothesis를 다르게 함\n",
        "def logit_fn(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "def hypothesis(X):\n",
        "    return tf.nn.softmax(logit_fn(X))\n",
        "\n",
        "def cost_fn(X, Y):\n",
        "    logits = logit_fn(X)\n",
        "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits,\n",
        "                                                      from_logits=True)\n",
        "    cost = tf.reduce_mean(cost_i)\n",
        "    return cost\n",
        "\n",
        "#이전과 동일\n",
        "def grad_fn(X, Y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = cost_fn(X, Y)\n",
        "        grads = tape.gradient(loss, variables)\n",
        "        return grads\n",
        "\n",
        "#정확도를 나타내주는것이 추가됨\n",
        "#tf.argmax 알아보기\n",
        "def prediction(X, Y):\n",
        "    pred = tf.argmax(hypothesis(X), 1)\n",
        "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "XK2E4nTL8q_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, Y, epochs=1000, verbose=100):\n",
        "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        grads = grad_fn(X, Y)\n",
        "        optimizer.apply_gradients(zip(grads, variables))\n",
        "        if (i==0) | ((i+1)%verbose==0):\n",
        "#             print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
        "            acc = prediction(X, Y).numpy()\n",
        "            loss = cost_fn(X, Y).numpy()\n",
        "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
        "\n",
        "fit(x_data, Y_one_hot)"
      ],
      "metadata": {
        "id": "feA9A4S9B4-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 07-1: Learning Rate and Evaluation**"
      ],
      "metadata": {
        "id": "bd6jxTihCFG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Learning Rate**  \n",
        "&rarr; Learnging Rate 값이 크면 Overshootiing 현상이 생긴다. 즉, 다음 weight 값이 최소 점보다 더 멀리 나아가 발산한다.  \n",
        "&rarr; Laerning Rate 값이 작으면 시간이 오래 걸려 Overfitting이나 발산형태로 나아간다.  \n"
      ],
      "metadata": {
        "id": "PqnbnoSXD9KX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Learning Rate Decay**  \n",
        "&rarr; Learning Rate를 일정 epoch 마다 값을 바꿔주는 과정  \n",
        "$\\alpha = \\frac{\\alpha_{0}}{1+kt}$  \n",
        "$\\alpha = \\alpha_{0}e^{-kt}$  \n"
      ],
      "metadata": {
        "id": "CqOKjHQsgY5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [[1, 2, 1],\n",
        "          [1, 3, 2],\n",
        "          [1, 3, 4],\n",
        "          [1, 5, 5],\n",
        "          [1, 7, 5],\n",
        "          [1, 2, 5],\n",
        "          [1, 6, 6],\n",
        "          [1, 7, 7]]\n",
        "\n",
        "y_train = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "# Evaluation our model using this test dataset\n",
        "x_test = [[2, 1, 1],\n",
        "          [3, 1, 2],\n",
        "          [3, 3, 4]]\n",
        "y_test = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1]]\n",
        "\n",
        "\n",
        "x1 = [x[0] for x in x_train]\n",
        "x2 = [x[1] for x in x_train]\n",
        "x3 = [x[2] for x in x_train]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, x3, c=y_train, marker='^')\n",
        "\n",
        "ax.scatter(x_test[0][0], x_test[0][1], x_test[0][2], c=\"black\", marker='^')\n",
        "ax.scatter(x_test[1][0], x_test[1][1], x_test[1][2], c=\"black\", marker='^')\n",
        "ax.scatter(x_test[2][0], x_test[2][1], x_test[2][2], c=\"black\", marker='^')\n",
        "\n",
        "\n",
        "ax.set_xlabel('X Label')\n",
        "ax.set_ylabel('Y Label')\n",
        "ax.set_zlabel('Z Label')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HCx2tn2GB7mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **필요한 함수 정의**"
      ],
      "metadata": {
        "id": "Q0oMviJBw6b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()\n",
        "\n",
        "W = tf.Variable(tf.random.normal((3, 3)))\n",
        "b = tf.Variable(tf.random.normal((3,)))\n",
        "\n",
        "def softmax_fn(features):\n",
        "    hypothesis = tf.nn.softmax(tf.matmul(features, W) + b)\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, features, labels):\n",
        "    cost = tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(hypothesis), axis=1))\n",
        "    return cost"
      ],
      "metadata": {
        "id": "4i8tWQA0v_rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Define Accuracy**"
      ],
      "metadata": {
        "id": "rvQ1ku52xa_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    prediction = tf.argmax(hypothesis, 1)\n",
        "    is_correct = tf.equal(prediction, tf.argmax(labels, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(softmax_fn(features),features,labels)\n",
        "    return tape.gradient(loss_value, [W,b])"
      ],
      "metadata": {
        "id": "lwsqOAV5xfn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0: Exponential Decay\n",
        "1: Inverse Time Decay\n",
        "2: Cosine Daecay\n",
        "3: Piecewise Decay\n",
        "\n",
        "\"\"\"\n",
        "def learningRate(command):\n",
        "    if command ==0:\n",
        "\n",
        "        learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.1,\n",
        "                                                                    decay_steps=100,\n",
        "                                                                    decay_rate=0.96,\n",
        "                                                                    staircase=True)\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "    elif command == 1:\n",
        "        learning_rate = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=0.1,\n",
        "                                                                   decay_steps=100,\n",
        "                                                                   decay_rate=0.96,\n",
        "                                                                   staircase=True)\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "    elif command == 2:\n",
        "        learning_rate = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.1,\n",
        "                                                    decay_steps=100,\n",
        "                                                    alpha=0.0)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "    elif command == 3:\n",
        "        boundaries = [300, 800]\n",
        "        values = [1.0, 0.5, 0.1]\n",
        "        learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mnfRWHGiw_8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Exponential Decay**  \n",
        "- $\\alpha = \\alpha_{0}e^{-kt}$  \n"
      ],
      "metadata": {
        "id": "q0_7NYW62ioE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0: Exponential Decay\n",
        "1: Inverse Time Decay\n",
        "2: Cosine Daecay\n",
        "3: Piecewise Decay\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        grads = grad(softmax_fn(features), features, labels)\n",
        "        optimizer = learningRate(0)\n",
        "\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(softmax_fn(features),features,labels)))\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "y_test = tf.cast(y_test, tf.float32)\n",
        "test_acc = accuracy_fn(softmax_fn(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "Ucg_MFDwkal_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Inverse Time Decay**  \n",
        "&rarr; $\\alpha = \\frac{\\alpha_{0}}{1+kt}$"
      ],
      "metadata": {
        "id": "oACuuuOr3aws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0: Exponential Decay\n",
        "1: Inverse Time Decay\n",
        "2: Cosine Daecay\n",
        "3: Piecewise Decay\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        grads = grad(softmax_fn(features), features, labels)\n",
        "        optimizer = learningRate(1)\n",
        "\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(softmax_fn(features),features,labels)))\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "y_test = tf.cast(y_test, tf.float32)\n",
        "test_acc = accuracy_fn(softmax_fn(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "nYzWIj5AkYeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Cosine Annealing**  \n",
        "-  $\\alpha = \\alpha_{min}^{i} + \\frac{1}{2}(\\alpha_{max}^{i} - \\alpha_{min}^{i})(1+ \\cos(\\frac{T_{current}}{T_{i}}\\pi))$  \n",
        "-  $\\alpha_{min}, \\alpha_{max}$: 학습전 설정된 learning rate의 최대 최소값  \n",
        "- $T_{current}$: 현재 Epoch  \n",
        "- $T_{i}$: Cosine Annealing을 실행하는 주기    "
      ],
      "metadata": {
        "id": "FtqRvyKr4bw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0: Exponential Decay\n",
        "1: Inverse Time Decay\n",
        "2: Cosine Daecay\n",
        "3: Piecewise Decay\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        grads = grad(softmax_fn(features), features, labels)\n",
        "        optimizer = learningRate(2)\n",
        "\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(softmax_fn(features),features,labels)))\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "y_test = tf.cast(y_test, tf.float32)\n",
        "test_acc = accuracy_fn(softmax_fn(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "Bxf8tkkQkVN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Piecewise Annealing**  \n",
        "- 특정 Epoch에 도달할 때 특정 값을 learning rate로 바꾼다  \n",
        "- **keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries, values)**  \n",
        "- boudary와 value는 순서가 있는 객체로 선언해야함"
      ],
      "metadata": {
        "id": "MvVuTglIFU0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0: Exponential Decay\n",
        "1: Inverse Time Decay\n",
        "2: Cosine Daecay\n",
        "3: Piecewise Decay\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        grads = grad(softmax_fn(features), features, labels)\n",
        "        optimizer = learningRate(3)\n",
        "\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(softmax_fn(features),features,labels)))\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "y_test = tf.cast(y_test, tf.float32)\n",
        "test_acc = accuracy_fn(softmax_fn(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "Y7-LOeYrkiMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Model Fitting**"
      ],
      "metadata": {
        "id": "w5AAzvudA_w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0: Exponential Decay\n",
        "1: Inverse Time Decay\n",
        "2: Cosine Daecay\n",
        "3: Piecewise Decay\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        grads = grad(softmax_fn(features), features, labels)\n",
        "        optimizer = learningRate(3)\n",
        "\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(softmax_fn(features),features,labels)))\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "y_test = tf.cast(y_test, tf.float32)\n",
        "test_acc = accuracy_fn(softmax_fn(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "utzDmlXmxqD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 07-2: linear regression(without min/max)**"
      ],
      "metadata": {
        "id": "DD7g4pVzGxBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Normalization**  \n",
        "&rarr; 데이터의 값을 0과1 사이로 만들어주는 과정  \n",
        "$x_{new} = \\frac{x - \\mu}{\\sigma}$  \n",
        ">**Standardization**  \n",
        "&rarr; 평균과 얼마나 떨어져 있는지에 대해서 평균에 대해 정규화 하는 과정  \n",
        "$x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$"
      ],
      "metadata": {
        "id": "7FZ_UcIdmMfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalization(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    return numerator / denominator\n",
        "\n",
        "def standardization(data):\n",
        "    numerator = data - np.mean(data)\n",
        "    denominator = sqrt(np.sum(data - np.mean(data))^2/np.count(data))"
      ],
      "metadata": {
        "id": "udKsIiphmhMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 07-3: Overfitting**"
      ],
      "metadata": {
        "id": "M6vZFlff8dN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Overfitting**  \n",
        "&rarr; model이 너무 train data에 취중해서 fitting 됨  \n"
      ],
      "metadata": {
        "id": "TPhjHqbc-lW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Over fitting 줄이는 방법**  \n",
        "- train data를 많이 받는다  \n",
        "- feature 수를 줄인다 (차원 축소)  \n",
        "- feature 수를 늘린다 (hypothesis의 식을 더 많이)"
      ],
      "metadata": {
        "id": "FjjE1rdi_fvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Data Set and Validation**  \n",
        "&rarr; train data와 test data의 비율을 잘 조정해서 data를 구성해야 한다  \n"
      ],
      "metadata": {
        "id": "rK75uEN8Dlfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Fine tuning**  \n",
        "&rarr; model을 Learning 하는 과정에서 모델의 특정 분류 방법을 고치거나 기존 모델에 또 다른 기법을 추가해서 fitting 하는 과정"
      ],
      "metadata": {
        "id": "bN22nhTTEdoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 07-3-2: Mnist**"
      ],
      "metadata": {
        "id": "_YYZ_psBBnoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Mnist data set**  \n",
        "&rarr; 0부터 9까지 모아 놓은 손글씨 data set"
      ],
      "metadata": {
        "id": "MohL4JXJByv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "2w5r4a6gmWUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Model**"
      ],
      "metadata": {
        "id": "dK4XZUwhCCYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "NFN73Jc5B8-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Optimizer, Cross Entropy**"
      ],
      "metadata": {
        "id": "F84amTiFCKAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BHhVio22CBoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Model fitting**"
      ],
      "metadata": {
        "id": "FDmmkgdJCUvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "GufpXluZCRKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Evaluation**"
      ],
      "metadata": {
        "id": "w8_85VFNCZd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "Ks09f7oHCZHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Fasion mnist**  \n",
        "&rarr; 10가지의 옷의 labe을 가진 data"
      ],
      "metadata": {
        "id": "IgHfDBy-FAiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "id": "tsIeKN1KFPF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[3])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])"
      ],
      "metadata": {
        "id": "F5cGK9BwFR2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}