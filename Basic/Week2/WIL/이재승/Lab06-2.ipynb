{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2dq/5mPPffJ/obVaP3M0N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"naTEpIouemt8","executionInfo":{"status":"ok","timestamp":1695043948795,"user_tz":-540,"elapsed":2,"user":{"displayName":"이재승","userId":"06759334452558132573"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","tf.random.set_seed(777)"]},{"cell_type":"code","source":["# 동물의 특징 16개를 가지는 dataset, 데이터 나누는 기준은 ,\n","xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, -1]\n","\n","nb_classes = 7  # 0 ~ 6\n","\n","# one-hot encoding\n","'''\n","# 이전 코드\n","Y_one_hot = tf.one_hot(list(y_data), nb_classes)\n","Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n","# one_hot()을 거치면 rank가 N -> N+1로 올라감(데이터를 한번 더 감쌈)\n","'''\n","Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n","\n","print(x_data.shape, Y_one_hot.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SnjE7TFhfJ5w","executionInfo":{"status":"ok","timestamp":1695045876847,"user_tz":-540,"elapsed":341,"user":{"displayName":"이재승","userId":"06759334452558132573"}},"outputId":"219332eb-b0eb-44fc-f524-3cda0d88b622"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(101, 16) (101, 7)\n"]}]},{"cell_type":"code","source":["W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n","b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n","variables = [W, b]"],"metadata":{"id":"Uk02h6IAfk5T","executionInfo":{"status":"ok","timestamp":1695045460865,"user_tz":-540,"elapsed":343,"user":{"displayName":"이재승","userId":"06759334452558132573"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def logit_fn(X):\n","    return tf.matmul(X, W) + b\n","\n","def hypothesis(X):\n","    return tf.nn.softmax(logit_fn(X))\n","\n","# logit과 hypothesis를 분리한 이유 : cross entropy 함수를 사용하기 위함.\n","# hypothesis 굳이 사용할 이유가 없는데(함수에서 해주기 때문) 쓴 이유 : prediction에서 사용\n","\n","def cost_fn(X, Y):\n","    logits = logit_fn(X)\n","    # 근데 버전 업그레이드로 인해 다른 함수를 사용, 추가 조사 필요.\n","    # 이도 강의에 나온 함수 처럼 logits을 사용하고 이를 from_logits로 표현하고 있음.\n","    cost_i = tf.keras.losses.categorical_crossentropy(\n","        y_true=Y, y_pred=logits, from_logits=True)\n","    cost = tf.reduce_mean(cost_i)\n","    return cost\n","\n","def grad_fn(X, Y):\n","    with tf.GradientTape() as tape:\n","        loss = cost_fn(X, Y)\n","        grads = tape.gradient(loss, variables)\n","        return grads\n","\n","def prediction(X, Y):\n","    pred = tf.argmax(hypothesis(X), 1)\n","    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","    return accuracy"],"metadata":{"id":"0vK8Wyi1lR6d","executionInfo":{"status":"ok","timestamp":1695046214044,"user_tz":-540,"elapsed":373,"user":{"displayName":"이재승","userId":"06759334452558132573"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def fit(X, Y, epochs=1000, verbose=100):\n","    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n","\n","    for i in range(epochs):\n","        grads = grad_fn(X, Y)\n","        optimizer.apply_gradients(zip(grads, variables))\n","        if (i == 0) | ((i + 1) % verbose == 0):\n","            acc = prediction(X, Y).numpy()\n","            loss = cost_fn(X, Y).numpy()\n","            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n","\n","fit(x_data, Y_one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJ44k8bnlcIW","executionInfo":{"status":"ok","timestamp":1695046225788,"user_tz":-540,"elapsed":9368,"user":{"displayName":"이재승","userId":"06759334452558132573"}},"outputId":"27d81229-f462-4a31-e0e6-3b64dd706169"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Steps: 1 Loss: 0.09009025990962982, Acc: 1.0\n","Steps: 100 Loss: 0.08323754370212555, Acc: 1.0\n","Steps: 200 Loss: 0.07733815908432007, Acc: 1.0\n","Steps: 300 Loss: 0.07224830985069275, Acc: 1.0\n","Steps: 400 Loss: 0.06780780106782913, Acc: 1.0\n","Steps: 500 Loss: 0.06389689445495605, Acc: 1.0\n","Steps: 600 Loss: 0.0604240745306015, Acc: 1.0\n","Steps: 700 Loss: 0.05731821060180664, Acc: 1.0\n","Steps: 800 Loss: 0.054522931575775146, Acc: 1.0\n","Steps: 900 Loss: 0.0519932396709919, Acc: 1.0\n","Steps: 1000 Loss: 0.04969245567917824, Acc: 1.0\n"]}]}]}