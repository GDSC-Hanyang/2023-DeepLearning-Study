{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a082bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### matplotlib 한글 폰트 설정 #############################\n",
    "from matplotlib import font_manager, rc\n",
    "## for window #####\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "## for mac #####\n",
    "#rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa32b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868116d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f07179d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "358da08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4658a701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7892ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f579e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d5e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # attention을 위해 이부분 추가\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24cb2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "#summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f81a8e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.0395 Batch Loss 0.9864\n",
      "Epoch 10 Loss 0.0374 Batch Loss 0.9359\n",
      "Epoch 20 Loss 0.0341 Batch Loss 0.8513\n",
      "Epoch 30 Loss 0.0323 Batch Loss 0.8087\n",
      "Epoch 40 Loss 0.0293 Batch Loss 0.7321\n",
      "Epoch 50 Loss 0.0225 Batch Loss 0.5625\n",
      "Epoch 60 Loss 0.0149 Batch Loss 0.3737\n",
      "Epoch 70 Loss 0.0094 Batch Loss 0.2338\n",
      "Epoch 80 Loss 0.0055 Batch Loss 0.1386\n",
      "Epoch 90 Loss 0.0030 Batch Loss 0.0761\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        #checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb2ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa31d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f5778d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd548895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x130cceaeb90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30d14f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I feel hungry\n",
      "Predicted translation: 나는 배가 고프다 <eos> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_31052\\973134332.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_31052\\973134332.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I feel hungry'\n",
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c424f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
