{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2085d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### matplotlib 한글 폰트 설정 #############################\n",
    "from matplotlib import font_manager, rc\n",
    "## for window #####\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "## for mac #####\n",
    "#rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a47480f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e752d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b29e8a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb0873e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b36fab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa3f4361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6322dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)\n",
    "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e67f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd62a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "470fa01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c30c4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07021f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.0395 Batch Loss 0.9884\n",
      "Epoch 10 Loss 0.0374 Batch Loss 0.9347\n",
      "Epoch 20 Loss 0.0342 Batch Loss 0.8557\n",
      "Epoch 30 Loss 0.0328 Batch Loss 0.8192\n",
      "Epoch 40 Loss 0.0304 Batch Loss 0.7588\n",
      "Epoch 50 Loss 0.0246 Batch Loss 0.6139\n",
      "Epoch 60 Loss 0.0173 Batch Loss 0.4322\n",
      "Epoch 70 Loss 0.0117 Batch Loss 0.2922\n",
      "Epoch 80 Loss 0.0073 Batch Loss 0.1819\n",
      "Epoch 90 Loss 0.0042 Batch Loss 0.1051\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1c5ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4a6ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "469e9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a77795e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1c8e7aea850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd57065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I feel hungry\n",
      "Predicted translation: 나는 배가 고프다 <eos> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jihyu\\AppData\\Local\\Temp\\ipykernel_14880\\3266044770.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "C:\\Users\\jihyu\\AppData\\Local\\Temp\\ipykernel_14880\\3266044770.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I feel hungry'\n",
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
