{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20415f9-4a3b-47f1-8667-2c474c3e0b08",
   "metadata": {},
   "source": [
    "# 2. Simple Linear Regression\n",
    "- 단순 선형 회귀라고도 부르며, 가장 단순하기 때문에 가장 기본이 되는 회귀 알고리즘이다.\n",
    "- 하나의 종속 변수 $y$에 대해 하나의 독립 변수 $x$를 가지는 모델이며, 식으로는 $y=ax+b$또는 $H(x)=Wx+b$로 나타낸다.\n",
    "- 데이터들을 대변하는 하나의 선(가설 함수)을 $H(x)$으로 표현하였을 때, $H(x)-y$를 $Cost$ or $Loss$ or $Error$라고 부른다\n",
    "- 얼마나 데이터들을 잘 대변했는지를 측정하는 비용 함수를 정의하는 방법은 여러 가지 있으며, 대표적으로는 $cost(W,b) = {1 \\over m}\\sum_{i=1}^m (H(x_i)-y_i)^2$로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7090cf8-9476-4640-8e44-d19ab19d49b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e484a28b-422f-47d9-aa39-18b2d51fcd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 8.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "plt.plot(x_data, y_data, 'o')\n",
    "plt.ylim(0, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c2c03-0e96-4257-b570-859ba2db8d1f",
   "metadata": {},
   "source": [
    "- _tf.reduce_mean_: Computes the mean of elements across dimensions of a tensor.\n",
    "- _tf.square_: Computes square of x element-wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e03b734e-5998-4a75-bde2-d8b0d70926a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=2.5>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=9>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v =[1., 2., 3., 4.]\n",
    "tf.reduce_mean(v), tf.square(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f0062-efab-4d8d-a335-7b1a8e6e856a",
   "metadata": {},
   "source": [
    "- _tf.Variable_: A variable maintains shared, persistent state manipulated by a program. (해당 변수는 프로그램이 조작하는 공유 지속 상태를 유지합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16118e02-0cd6-4f8b-be82-accb7c048b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(2.0)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "hypothesis = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c43cdb93-6040-44bf-ba21-f318e19989d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 0.5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array로 변환\n",
    "W.numpy(), b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737aa5d-25ec-4265-8e82-85db094d33c4",
   "metadata": {},
   "source": [
    "- hypothesis는 계산에 사용된 W, b와 다른 타입을 가짐 (각각 EagerTensor, ResourceVariable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d19f3c7d-8b66-461b-9d3e-e6178662a0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.ops.resource_variable_ops.ResourceVariable)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hypothesis), type(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40454b4-c18a-4162-841d-d649a1e92f2a",
   "metadata": {},
   "source": [
    "- $ cost = {1 \\over m}\\sum_{i=1}^m (H(x_i)-y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750970b4-0ae3-4761-8533-130b497acbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf506b64-e3ba-4df7-936a-3e24df83d282",
   "metadata": {},
   "source": [
    "## Gradient descent (경사하강법)\n",
    "- 함수의 기울기(경사)를 구한 뒤, 경사의 반대 방향으로 iterative하게 이동시켜 extreme point(극값)을 찾는 방법이다.\n",
    "- 이는 딥러닝 학습 시 사용되는 최적화 방법 중 하나이지만, Global이 아닌 local extrema point에 수렴할 수 있으며 학습률에 따라 발산 또는 긴 수렴 시간을 가지는 등의 문제점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526efb1-7127-4cc8-8628-b38d60fb1c29",
   "metadata": {},
   "source": [
    "- tf.GradientTape(persistent=False, watch_accessed_variables=True)<br>\n",
    "=> Record operations for _automatic differentiation_ (자동 미분)<br>\n",
    "=> Operations are recorded if they are executed within this context manager and \n",
    "at least one of their inputs is being  \"watched\". (작업이 이 컨텍스트 관리자 내에서 실행되고 입력 중 하나 이상이 \"관측\"되는 경우 기록됩니다.).<br>\n",
    "=> 중간 연산 과정을 tape에 저장하고 이후 gradient를 연산하도록 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dabd987c-0858-449a-996e-af6507ee93dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tf_export(\"GradientTape\", \"autodiff.GradientTape\", v1=[\"GradientTape\"])\n",
      "class GradientTape:\n",
      "  \"\"\"Record operations for automatic differentiation.\n",
      "\n",
      "  Operations are recorded if they are executed within this context manager and\n",
      "  at least one of their inputs is being \"watched\".\n",
      "\n",
      "  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n",
      "  where `trainable=True` is default in both cases) are automatically watched.\n",
      "  Tensors can be manually watched by invoking the `watch` method on this context\n",
      "  manager.\n",
      "\n",
      "  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n",
      "  be computed as:\n",
      "\n",
      "  >>> x = tf.constant(3.0)\n",
      "  >>> with tf.GradientTape() as g:\n",
      "  ...   g.watch(x)\n",
      "  ...   y = x * x\n",
      "  >>> dy_dx = g.gradient(y, x)\n",
      "  >>> print(dy_dx)\n",
      "  tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "\n",
      "  GradientTapes can be nested to compute higher-order derivatives. For example,\n",
      "\n",
      "  >>> x = tf.constant(5.0)\n",
      "  >>> with tf.GradientTape() as g:\n",
      "  ...   g.watch(x)\n",
      "  ...   with tf.GradientTape() as gg:\n",
      "  ...     gg.watch(x)\n",
      "  ...     y = x * x\n",
      "  ...   dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n",
      "  >>> d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n",
      "  >>> print(dy_dx)\n",
      "  tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "  >>> print(d2y_dx2)\n",
      "  tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "\n",
      "  By default, the resources held by a GradientTape are released as soon as\n",
      "  GradientTape.gradient() method is called. To compute multiple gradients over\n",
      "  the same computation, create a persistent gradient tape. This allows multiple\n",
      "  calls to the gradient() method as resources are released when the tape object\n",
      "  is garbage collected. For example:\n",
      "\n",
      "  >>> x = tf.constant(3.0)\n",
      "  >>> with tf.GradientTape(persistent=True) as g:\n",
      "  ...   g.watch(x)\n",
      "  ...   y = x * x\n",
      "  ...   z = y * y\n",
      "  >>> dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n",
      "  >>> print(dz_dx)\n",
      "  tf.Tensor(108.0, shape=(), dtype=float32)\n",
      "  >>> dy_dx = g.gradient(y, x)\n",
      "  >>> print(dy_dx)\n",
      "  tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "\n",
      "  By default GradientTape will automatically watch any trainable variables that\n",
      "  are accessed inside the context. If you want fine grained control over which\n",
      "  variables are watched you can disable automatic tracking by passing\n",
      "  `watch_accessed_variables=False` to the tape constructor:\n",
      "\n",
      "  >>> x = tf.Variable(2.0)\n",
      "  >>> w = tf.Variable(5.0)\n",
      "  >>> with tf.GradientTape(\n",
      "  ...     watch_accessed_variables=False, persistent=True) as tape:\n",
      "  ...   tape.watch(x)\n",
      "  ...   y = x ** 2  # Gradients will be available for `x`.\n",
      "  ...   z = w ** 3  # No gradients will be available as `w` isn't being watched.\n",
      "  >>> dy_dx = tape.gradient(y, x)\n",
      "  >>> print(dy_dx)\n",
      "  tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "  >>> # No gradients will be available as `w` isn't being watched.\n",
      "  >>> dz_dw = tape.gradient(z, w)\n",
      "  >>> print(dz_dw)\n",
      "  None\n",
      "\n",
      "  Note that when using models you should ensure that your variables exist when\n",
      "  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n",
      "  first iteration not have any gradients:\n",
      "\n",
      "  ```python\n",
      "  a = tf.keras.layers.Dense(32)\n",
      "  b = tf.keras.layers.Dense(32)\n",
      "\n",
      "  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
      "                             # `a.variables` will return an empty list and the\n",
      "                             # tape will not be watching anything.\n",
      "    result = b(a(inputs))\n",
      "    tape.gradient(result, a.variables)  # The result of this computation will be\n",
      "                                        # a list of `None`s since a's variables\n",
      "                                        # are not being watched.\n",
      "  ```\n",
      "\n",
      "  Note that only tensors with real or complex dtypes are differentiable.\n",
      "  \"\"\"\n",
      "\n",
      "  def __init__(self, persistent=False, watch_accessed_variables=True):\n",
      "    \"\"\"Creates a new GradientTape.\n",
      "\n",
      "    Args:\n",
      "      persistent: Boolean controlling whether a persistent gradient tape\n",
      "        is created. False by default, which means at most one call can\n",
      "        be made to the gradient() method on this object.\n",
      "      watch_accessed_variables: Boolean controlling whether the tape will\n",
      "        automatically `watch` any (trainable) variables accessed while the tape\n",
      "        is active. Defaults to True meaning gradients can be requested from any\n",
      "        result computed in the tape derived from reading a trainable `Variable`.\n",
      "        If False users must explicitly `watch` any `Variable`s they want to\n",
      "        request gradients from.\n",
      "    \"\"\"\n",
      "    self._tape = None\n",
      "    self._persistent = persistent\n",
      "    self._watch_accessed_variables = watch_accessed_variables\n",
      "    self._watched_variables = ()\n",
      "    self._recording = False\n",
      "\n",
      "  def __enter__(self):\n",
      "    \"\"\"Enters a context inside which operations are recorded on this tape.\"\"\"\n",
      "    self._push_tape()\n",
      "    return self\n",
      "\n",
      "  def __exit__(self, typ, value, traceback):\n",
      "    \"\"\"Exits the recording context, no further operations are traced.\"\"\"\n",
      "    if self._recording:\n",
      "      self._pop_tape()\n",
      "\n",
      "  def _push_tape(self):\n",
      "    \"\"\"Pushes a new tape onto the tape stack.\"\"\"\n",
      "    if self._recording:\n",
      "      raise ValueError(\"Tape is still recording, This can happen if you try to \"\n",
      "                       \"re-enter an already-active tape.\")\n",
      "    if self._tape is None:\n",
      "      self._tape = tape.push_new_tape(\n",
      "          persistent=self._persistent,\n",
      "          watch_accessed_variables=self._watch_accessed_variables)\n",
      "    else:\n",
      "      tape.push_tape(self._tape)\n",
      "    self._recording = True\n",
      "\n",
      "  def _pop_tape(self):\n",
      "    if not self._recording:\n",
      "      raise ValueError(\"Tape is not recording.\")\n",
      "    tape.pop_tape(self._tape)\n",
      "    self._recording = False\n",
      "\n",
      "  @tf_contextlib.contextmanager\n",
      "  def _ensure_recording(self):\n",
      "    \"\"\"Ensures that this tape is recording.\"\"\"\n",
      "    if not self._recording:\n",
      "      try:\n",
      "        self._push_tape()\n",
      "        yield\n",
      "      finally:\n",
      "        self._pop_tape()\n",
      "    else:\n",
      "      yield\n",
      "\n",
      "  # TODO(b/209081027): Add a variable in composite tensor test case after\n",
      "  # variables become composite tensors.\n",
      "  def watch(self, tensor):\n",
      "    \"\"\"Ensures that `tensor` is being traced by this tape.\n",
      "\n",
      "    Args:\n",
      "      tensor: a Tensor/Variable or list of Tensors/Variables.\n",
      "\n",
      "    Raises:\n",
      "      ValueError: if it encounters something that is not a tensor.\n",
      "    \"\"\"\n",
      "    for t in _extract_tensors_and_variables(tensor):\n",
      "      if not backprop_util.IsTrainable(t):\n",
      "        logging.log_first_n(\n",
      "            logging.WARN, \"The dtype of the watched tensor must be \"\n",
      "            \"floating (e.g. tf.float32), got %r\", 5, t.dtype)\n",
      "      if hasattr(t, \"handle\"):\n",
      "        # There are many variable-like objects, all of them currently have\n",
      "        # `handle` attribute that points to a tensor. If this changes,\n",
      "        # internals of watch_variable need to change as well.\n",
      "        tape.watch_variable(self._tape, t)\n",
      "      else:\n",
      "        tape.watch(self._tape, t)\n",
      "\n",
      "  @tf_contextlib.contextmanager\n",
      "  def stop_recording(self):\n",
      "    \"\"\"Temporarily stops recording operations on this tape.\n",
      "\n",
      "    Operations executed while this context manager is active will not be\n",
      "    recorded on the tape. This is useful for reducing the memory used by tracing\n",
      "    all computations.\n",
      "\n",
      "    For example:\n",
      "\n",
      "    >>> x = tf.constant(4.0)\n",
      "    >>> with tf.GradientTape() as tape:\n",
      "    ...   with tape.stop_recording():\n",
      "    ...     y = x ** 2\n",
      "    >>> dy_dx = tape.gradient(y, x)\n",
      "    >>> print(dy_dx)\n",
      "    None\n",
      "\n",
      "    Yields:\n",
      "      None\n",
      "    Raises:\n",
      "      RuntimeError: if the tape is not currently recording.\n",
      "    \"\"\"\n",
      "    if self._tape is None:\n",
      "      raise RuntimeError(\n",
      "          \"Trying to stop recording a tape which is not recording.\")\n",
      "    self._pop_tape()\n",
      "    try:\n",
      "      yield\n",
      "    finally:\n",
      "      self._push_tape()\n",
      "\n",
      "  def reset(self):\n",
      "    \"\"\"Clears all information stored in this tape.\n",
      "\n",
      "    Equivalent to exiting and reentering the tape context manager with a new\n",
      "    tape. For example, the two following code blocks are equivalent:\n",
      "\n",
      "    ```\n",
      "    with tf.GradientTape() as t:\n",
      "      loss = loss_fn()\n",
      "    with tf.GradientTape() as t:\n",
      "      loss += other_loss_fn()\n",
      "    t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n",
      "\n",
      "\n",
      "    # The following is equivalent to the above\n",
      "    with tf.GradientTape() as t:\n",
      "      loss = loss_fn()\n",
      "      t.reset()\n",
      "      loss += other_loss_fn()\n",
      "    t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n",
      "    ```\n",
      "\n",
      "    This is useful if you don't want to exit the context manager for the tape,\n",
      "    or can't because the desired reset point is inside a control flow construct:\n",
      "\n",
      "    ```\n",
      "    with tf.GradientTape() as t:\n",
      "      loss = ...\n",
      "      if loss > k:\n",
      "        t.reset()\n",
      "    ```\n",
      "    \"\"\"\n",
      "    self._pop_tape()\n",
      "    self._tape = None\n",
      "    self._push_tape()\n",
      "\n",
      "  def watched_variables(self):\n",
      "    \"\"\"Returns variables watched by this tape in order of construction.\"\"\"\n",
      "    if self._tape is not None:\n",
      "      self._watched_variables = self._tape.watched_variables()\n",
      "    return self._watched_variables\n",
      "\n",
      "  def gradient(self,\n",
      "               target,\n",
      "               sources,\n",
      "               output_gradients=None,\n",
      "               unconnected_gradients=UnconnectedGradients.NONE):\n",
      "    \"\"\"Computes the gradient using operations recorded in context of this tape.\n",
      "\n",
      "    Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
      "    compute one set of gradients (or jacobians).\n",
      "\n",
      "    In addition to Tensors, gradient also supports RaggedTensors. For example,\n",
      "\n",
      "    >>> x = tf.ragged.constant([[1.0, 2.0], [3.0]])\n",
      "    >>> with tf.GradientTape() as g:\n",
      "    ...   g.watch(x)\n",
      "    ...   y = x * x\n",
      "    >>> g.gradient(y, x)\n",
      "    <tf.RaggedTensor [[2.0, 4.0], [6.0]]>\n",
      "\n",
      "    Args:\n",
      "      target: a list or nested structure of Tensors or Variables or\n",
      "        CompositeTensors to be differentiated.\n",
      "      sources: a list or nested structure of Tensors or Variables or\n",
      "        CompositeTensors. `target` will be differentiated against elements in\n",
      "        `sources`.\n",
      "      output_gradients: a list of gradients, one for each differentiable\n",
      "        element of target. Defaults to None.\n",
      "      unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
      "        alters the value which will be returned if the target and sources are\n",
      "        unconnected. The possible values and effects are detailed in\n",
      "        'UnconnectedGradients' and it defaults to 'none'.\n",
      "\n",
      "    Returns:\n",
      "      a list or nested structure of Tensors (or IndexedSlices, or None, or\n",
      "      CompositeTensor), one for each element in `sources`. Returned structure\n",
      "      is the same as the structure of `sources`.\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called on a used, non-persistent tape.\n",
      "      RuntimeError: If called inside the context of the tape.\n",
      "      TypeError: If the target is a None object.\n",
      "      ValueError: If the target is a variable or if unconnected gradients is\n",
      "       called with an unknown value.\n",
      "    \"\"\"\n",
      "    if self._tape is None:\n",
      "      raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n",
      "                         \"compute one set of gradients (or jacobians)\")\n",
      "    if self._recording:\n",
      "      if not self._persistent:\n",
      "        self._pop_tape()\n",
      "      else:\n",
      "        logging.log_first_n(\n",
      "            logging.WARN, \"Calling GradientTape.gradient on a persistent \"\n",
      "            \"tape inside its context is significantly less \"\n",
      "            \"efficient than calling it outside the context (it \"\n",
      "            \"causes the gradient ops to be recorded on the \"\n",
      "            \"tape, leading to increased CPU and memory usage). \"\n",
      "            \"Only call GradientTape.gradient inside the \"\n",
      "            \"context if you actually want to trace the \"\n",
      "            \"gradient in order to compute higher order \"\n",
      "            \"derivatives.\", 1)\n",
      "\n",
      "    if target is None:\n",
      "      raise TypeError(\"Argument `target` should be a list or nested structure\"\n",
      "                      \" of Tensors, Variables or CompositeTensors to be \"\n",
      "                      \"differentiated, but received None.\")\n",
      "\n",
      "    flat_targets = []\n",
      "    for t in nest.flatten(target):\n",
      "      flat_targets.append(_handle_or_self(t))\n",
      "    flat_targets = composite_tensor_gradient.get_flat_tensors_for_gradients(\n",
      "        flat_targets)\n",
      "    for t in flat_targets:\n",
      "      if not backprop_util.IsTrainable(t):\n",
      "        logging.vlog(\n",
      "            1, \"The dtype of the target tensor must be \"\n",
      "            \"floating (e.g. tf.float32) when calling GradientTape.gradient, \"\n",
      "            \"got %r\", t.dtype)\n",
      "\n",
      "    flat_sources_raw = nest.flatten(sources)\n",
      "    flat_sources = []\n",
      "    for t in flat_sources_raw:\n",
      "      flat_sources.append(_handle_or_self(t))\n",
      "    flat_sources = composite_tensor_gradient.get_flat_tensors_for_gradients(\n",
      "        flat_sources)\n",
      "    for t in flat_sources:\n",
      "      if not backprop_util.IsTrainable(t):\n",
      "        logging.vlog(\n",
      "            1, \"The dtype of the source tensor must be \"\n",
      "            \"floating (e.g. tf.float32) when calling GradientTape.gradient, \"\n",
      "            \"got %r\", t.dtype)\n",
      "      if getattr(t, \"is_packed\", False):\n",
      "        raise ValueError(\n",
      "            \"GradientTape.gradient is not supported on packed EagerTensors yet.\"\n",
      "        )\n",
      "\n",
      "    if output_gradients is not None:\n",
      "      output_gradients = nest.flatten(\n",
      "          variable_utils.convert_variables_to_tensors(output_gradients))\n",
      "      output_gradients = (\n",
      "          composite_tensor_gradient.get_flat_tensors_for_gradients(\n",
      "              output_gradients))\n",
      "      output_gradients = [None if x is None else ops.convert_to_tensor(x)\n",
      "                          for x in output_gradients]\n",
      "\n",
      "    flat_grad = imperative_grad.imperative_grad(\n",
      "        self._tape,\n",
      "        flat_targets,\n",
      "        flat_sources,\n",
      "        output_gradients=output_gradients,\n",
      "        sources_raw=flat_sources_raw,\n",
      "        unconnected_gradients=unconnected_gradients)\n",
      "\n",
      "    if not self._persistent:\n",
      "      # Keep track of watched variables before setting tape to None\n",
      "      self._watched_variables = self._tape.watched_variables()\n",
      "      self._tape = None\n",
      "\n",
      "    flat_sources_raw = nest.map_structure(_handle_or_self, flat_sources_raw)\n",
      "    flat_grad = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n",
      "        flat_sources_raw, flat_grad)\n",
      "    grad = nest.pack_sequence_as(sources, flat_grad)\n",
      "    return grad\n",
      "\n",
      "  def jacobian(self,\n",
      "               target,\n",
      "               sources,\n",
      "               unconnected_gradients=UnconnectedGradients.NONE,\n",
      "               parallel_iterations=None,\n",
      "               experimental_use_pfor=True):\n",
      "    \"\"\"Computes the jacobian using operations recorded in context of this tape.\n",
      "\n",
      "    Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
      "    compute one set of gradients (or jacobians).\n",
      "\n",
      "    Note: By default the jacobian implementation uses parallel for (pfor), which\n",
      "    creates a tf.function under the hood for each jacobian call. For better\n",
      "    performance, and to avoid recompilation and vectorization rewrites on each\n",
      "    call, enclose GradientTape code in @tf.function.\n",
      "\n",
      "    See[wikipedia\n",
      "    article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant)\n",
      "    for the definition of a Jacobian.\n",
      "\n",
      "    Example usage:\n",
      "\n",
      "    ```python\n",
      "    with tf.GradientTape() as g:\n",
      "      x  = tf.constant([1.0, 2.0])\n",
      "      g.watch(x)\n",
      "      y = x * x\n",
      "    jacobian = g.jacobian(y, x)\n",
      "    # jacobian value is [[2., 0.], [0., 4.]]\n",
      "    ```\n",
      "\n",
      "    Args:\n",
      "      target: Tensor to be differentiated.\n",
      "      sources: a list or nested structure of Tensors or Variables. `target`\n",
      "        will be differentiated against elements in `sources`.\n",
      "      unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
      "        alters the value which will be returned if the target and sources are\n",
      "        unconnected. The possible values and effects are detailed in\n",
      "        'UnconnectedGradients' and it defaults to 'none'.\n",
      "      parallel_iterations: A knob to control how many iterations are dispatched\n",
      "        in parallel. This knob can be used to control the total memory usage.\n",
      "      experimental_use_pfor: If true, vectorizes the jacobian computation. Else\n",
      "        falls back to a sequential while_loop. Vectorization can sometimes fail\n",
      "        or lead to excessive memory usage. This option can be used to disable\n",
      "        vectorization in such cases.\n",
      "\n",
      "    Returns:\n",
      "      A list or nested structure of Tensors (or None), one for each element in\n",
      "      `sources`. Returned structure is the same as the structure of `sources`.\n",
      "      Note if any gradient is sparse (IndexedSlices), jacobian function\n",
      "      currently makes it dense and returns a Tensor instead. This may change in\n",
      "      the future.\n",
      "\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called on a used, non-persistent tape.\n",
      "      RuntimeError: If called on a non-persistent tape with eager execution\n",
      "        enabled and without enabling experimental_use_pfor.\n",
      "      ValueError: If vectorization of jacobian computation fails.\n",
      "    \"\"\"\n",
      "    if self._tape is None:\n",
      "      raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n",
      "                         \"compute one set of gradients (or jacobians)\")\n",
      "\n",
      "    flat_sources = nest.flatten(sources)\n",
      "    target_static_shape = target.shape\n",
      "    target_shape = array_ops.shape(target)\n",
      "    # Note that we push and pop the tape here and below. This is needed since we\n",
      "    # need gradients through the enclosed operations.\n",
      "    with self._ensure_recording():\n",
      "      target = array_ops.reshape(target, [-1])\n",
      "\n",
      "    def loop_fn(i):\n",
      "      with self._ensure_recording():\n",
      "        y = array_ops.gather(target, i)\n",
      "      return self.gradient(y, flat_sources,\n",
      "                           unconnected_gradients=unconnected_gradients)\n",
      "\n",
      "    try:\n",
      "      target_size = int(target.shape[0])\n",
      "    except TypeError:\n",
      "      target_size = array_ops.shape(target)[0]\n",
      "\n",
      "    if experimental_use_pfor:\n",
      "      try:\n",
      "        output = pfor_ops.pfor(loop_fn, target_size,\n",
      "                               parallel_iterations=parallel_iterations)\n",
      "      except ValueError as err:\n",
      "        raise ValueError(\n",
      "            \"Encountered an exception while vectorizing the \"\n",
      "            \"jacobian computation. Vectorization can be disabled by setting\"\n",
      "            \" experimental_use_pfor to False.\") from err\n",
      "    else:\n",
      "      if context.executing_eagerly() and not self._persistent:\n",
      "        raise RuntimeError(\n",
      "            \"GradientTape must be created with persistent=True\"\n",
      "            \" to compute the jacobian with eager execution enabled and with \"\n",
      "            \" experimental_use_pfor set to False.\")\n",
      "      output = pfor_ops.for_loop(\n",
      "          loop_fn, [target.dtype] * len(flat_sources), target_size,\n",
      "          parallel_iterations=parallel_iterations)\n",
      "\n",
      "    for i, out in enumerate(output):\n",
      "      if out is not None:\n",
      "        new_shape = array_ops.concat(\n",
      "            [target_shape, array_ops.shape(out)[1:]], axis=0)\n",
      "        out = array_ops.reshape(out, new_shape)\n",
      "        if context.executing_eagerly():\n",
      "          out.set_shape(target_static_shape.concatenate(flat_sources[i].shape))\n",
      "      output[i] = out\n",
      "\n",
      "    return nest.pack_sequence_as(sources, output)\n",
      "\n",
      "  def batch_jacobian(self,\n",
      "                     target,\n",
      "                     source,\n",
      "                     unconnected_gradients=UnconnectedGradients.NONE,\n",
      "                     parallel_iterations=None,\n",
      "                     experimental_use_pfor=True):\n",
      "    \"\"\"Computes and stacks per-example jacobians.\n",
      "\n",
      "    See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant)\n",
      "    for the definition of a Jacobian. This function is essentially an efficient\n",
      "    implementation of the following:\n",
      "\n",
      "    `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\n",
      "\n",
      "    Note that compared to `GradientTape.jacobian` which computes gradient of\n",
      "    each output value w.r.t each input value, this function is useful when\n",
      "    `target[i,...]` is independent of `source[j,...]` for `j != i`. This\n",
      "    assumption allows more efficient computation as compared to\n",
      "    `GradientTape.jacobian`. The output, as well as intermediate activations,\n",
      "    are lower dimensional and avoid a bunch of redundant zeros which would\n",
      "    result in the jacobian computation given the independence assumption.\n",
      "\n",
      "    Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
      "    compute one set of gradients (or jacobians).\n",
      "\n",
      "    Note: By default the batch_jacobian implementation uses parallel for (pfor),\n",
      "    which creates a tf.function under the hood for each batch_jacobian call.\n",
      "    For better performance, and to avoid recompilation and vectorization\n",
      "    rewrites on each call, enclose GradientTape code in @tf.function.\n",
      "\n",
      "\n",
      "    Example usage:\n",
      "\n",
      "    ```python\n",
      "    with tf.GradientTape() as g:\n",
      "      x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n",
      "      g.watch(x)\n",
      "      y = x * x\n",
      "    batch_jacobian = g.batch_jacobian(y, x)\n",
      "    # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n",
      "    ```\n",
      "\n",
      "    Args:\n",
      "      target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].\n",
      "        `target[i,...]` should only depend on `source[i,...]`.\n",
      "      source: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].\n",
      "      unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
      "        alters the value which will be returned if the target and sources are\n",
      "        unconnected. The possible values and effects are detailed in\n",
      "        'UnconnectedGradients' and it defaults to 'none'.\n",
      "      parallel_iterations: A knob to control how many iterations are dispatched\n",
      "        in parallel. This knob can be used to control the total memory usage.\n",
      "      experimental_use_pfor: If true, uses pfor for computing the Jacobian. Else\n",
      "        uses a tf.while_loop.\n",
      "\n",
      "    Returns:\n",
      "      A tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]`\n",
      "      is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked\n",
      "      per-example jacobians.\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called on a used, non-persistent tape.\n",
      "      RuntimeError: If called on a non-persistent tape with eager execution\n",
      "        enabled and without enabling experimental_use_pfor.\n",
      "      ValueError: If vectorization of jacobian computation fails or if first\n",
      "        dimension of `target` and `source` do not match.\n",
      "    \"\"\"\n",
      "    if self._tape is None:\n",
      "      raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\n",
      "                         \"compute one set of gradients (or jacobians)\")\n",
      "    target_shape = target.shape\n",
      "    if target_shape.rank is None:\n",
      "      dim = tensor_shape.Dimension(None)\n",
      "    else:\n",
      "      dim = target_shape.dims[0]\n",
      "    if not (target_shape.with_rank_at_least(2) and\n",
      "            source.shape.with_rank_at_least(2) and\n",
      "            dim.is_compatible_with(source.shape[0])):\n",
      "      raise ValueError(\n",
      "          \"Need first dimension of target shape (%s) and \"\n",
      "          \"source shape (%s) to match.\" % (target.shape, source.shape))\n",
      "    if target_shape.is_fully_defined():\n",
      "      batch_size = int(target_shape[0])\n",
      "      target_row_size = target_shape.num_elements() // batch_size\n",
      "    else:\n",
      "      target_shape = array_ops.shape(target)\n",
      "      batch_size = target_shape[0]\n",
      "      target_row_size = array_ops.size(target) // batch_size\n",
      "    source_shape = array_ops.shape(source)\n",
      "    # Flatten target to 2-D.\n",
      "    # Note that we push and pop the tape here and below. This is needed since we\n",
      "    # need gradients through the enclosed operations.\n",
      "    with self._ensure_recording():\n",
      "      with ops.control_dependencies(\n",
      "          [check_ops.assert_equal(batch_size, source_shape[0])]):\n",
      "        target = array_ops.reshape(target, [batch_size, target_row_size])\n",
      "\n",
      "    run_once = False\n",
      "\n",
      "    def loop_fn(i):\n",
      "      nonlocal run_once\n",
      "      if run_once and not self._persistent:\n",
      "        if parallel_iterations is not None:\n",
      "          raise RuntimeError(\n",
      "              \"GradientTape must be created with persistent=True\"\n",
      "              \" to compute the batch_jacobian with parallel_iterations.\")\n",
      "        else:\n",
      "          raise RuntimeError(\n",
      "              \"GradientTape must be created with persistent=True\"\n",
      "              \" to compute the batch_jacobian.\")\n",
      "      run_once = True\n",
      "\n",
      "      with self._ensure_recording():\n",
      "        y = array_ops.gather(target, i, axis=1)\n",
      "      return self.gradient(y, source,\n",
      "                           unconnected_gradients=unconnected_gradients)\n",
      "\n",
      "    if experimental_use_pfor:\n",
      "      try:\n",
      "        output = pfor_ops.pfor(loop_fn, target_row_size,\n",
      "                               parallel_iterations=parallel_iterations)\n",
      "      except ValueError as err:\n",
      "        raise ValueError(\n",
      "            \"Encountered an exception while vectorizing the \"\n",
      "            \"batch_jacobian computation. Vectorization can be disabled by \"\n",
      "            \"setting experimental_use_pfor to False.\") from err\n",
      "    else:\n",
      "      if context.executing_eagerly() and not self._persistent:\n",
      "        raise RuntimeError(\n",
      "            \"GradientTape must be created with persistent=True\"\n",
      "            \" to compute the batch_jacobian with eager execution enabled and \"\n",
      "            \" with experimental_use_pfor set to False.\")\n",
      "      output = pfor_ops.for_loop(loop_fn, target.dtype, target_row_size,\n",
      "                                 parallel_iterations=parallel_iterations)\n",
      "    new_shape = array_ops.concat([target_shape, source_shape[1:]], axis=0)\n",
      "    if output is None:\n",
      "      # Note that this block is returning zeros when it could use `None` to\n",
      "      # represent unconnected gradients. This is to maintain compatibility with\n",
      "      # the previous behavior, which ignored `unconnected_gradients`.\n",
      "      output = array_ops.zeros(new_shape, target.dtype)\n",
      "      return output\n",
      "    else:\n",
      "      output = array_ops.reshape(output,\n",
      "                                 [target_row_size, batch_size, -1])\n",
      "      output = array_ops.transpose(output, [1, 0, 2])\n",
      "\n",
      "      output = array_ops.reshape(output, new_shape)\n",
      "      return output\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "print(getsource(tf.GradientTape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19151ca-93ee-452d-800c-ac3b869b14ed",
   "metadata": {},
   "source": [
    "- 경사하강법을 **한 번** 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50fa2d5e-fd26-4065-96a0-ca707bc78454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경사하강법 전: W=2.0, b=0.5\n",
      "경사하강법 한 번 실행: W=1.75, b=0.4300000071525574\n"
     ]
    }
   ],
   "source": [
    "# 경사하강법을 한 번 수행\n",
    "learning_rate = 0.01\n",
    "\n",
    "W = tf.Variable(2.0)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    hypothesis = W * x_data + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "W_grad.numpy(), b_grad.numpy()\n",
    "\n",
    "print(f'경사하강법 전: W={W.numpy()}, b={b.numpy()}')\n",
    "\n",
    "W.assign_sub(learning_rate * W_grad)\n",
    "b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "print(f'경사하강법 한 번 실행: W={W.numpy()}, b={b.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa19e6-b2df-4114-908a-36656199ce52",
   "metadata": {},
   "source": [
    "- 경사하강법을 **여러 번** 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd777c8a-0eba-4813-a2da-90ad24f4b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    2.4520|    0.3760| 45.660004\n",
      "   10|    1.1036|    0.0034|  0.206336\n",
      "   20|    1.0128|   -0.0209|  0.001026\n",
      "   30|    1.0065|   -0.0218|  0.000093\n",
      "   40|    1.0059|   -0.0212|  0.000083\n",
      "   50|    1.0057|   -0.0205|  0.000077\n",
      "   60|    1.0055|   -0.0198|  0.000072\n",
      "   70|    1.0053|   -0.0192|  0.000067\n",
      "   80|    1.0051|   -0.0185|  0.000063\n",
      "   90|    1.0050|   -0.0179|  0.000059\n",
      "   99|    1.0048|   -0.0174|  0.000055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 8.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "# 경사하강법 100번 수행\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    # 10번 돌 때마다 W, b 값 확인\n",
    "    if i % 10 == 0:\n",
    "      print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "\n",
    "# 경사하강법을 100번 수행한 W, b\n",
    "print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "plt.plot(x_data, y_data, 'o')\n",
    "plt.plot(x_data, hypothesis.numpy(), 'r-')\n",
    "plt.ylim(0, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd283aa-c094-483a-ad1a-c0f0252649a4",
   "metadata": {},
   "source": [
    "## with 구문\n",
    "- open 등의 메소드로 파일을 열어 작업을 수행하였다면 반드시 close() 등의 메소드를 통해 파일을 닫아주어야만 하며, 만약 그렇지 않을 시 파일 리소스가 메모리를 계속 점유하게 된다.\n",
    "- 이는 자원 관리 측면에서 매우 비효율적이기 때문에 파이썬은 with 구문을 통해 이러한 문제를 해결하고자 하였다.\n",
    "- with 구문을 사용하면 파일 작업 이후 리소스의 메모리 점유를 삭제하므로 자원이 관리되며, open 메소드 이후 발생하는 예외에 대해서도 대처 가능하다.\n",
    "- with 구문은 작업의 실행을 컨텍스트 관리자가 정의한 메소드로 감싸는 역할을 하며, try-except-finally 사용 패턴을 편리하게 재사용할 수 있도록 캡슐화한다.\n",
    "- 러프하게 본다면 with 구문으로 작업을 수행할 시 컨텍스트 관리자에서 정의한 \\_\\_enter\\_\\_가 먼저 호출되고, 그 다음 작업이 수행되며, 작업 이후에는 \\_\\_exit\\_\\_가 호출되며 예외 발생 시 \\_\\_exit\\_\\_의 인자로 전달된다.\n",
    "- with 설명(파이썬 공식 문서): https://docs.python.org/ko/3/reference/compound_stmts.html#the-with-statement, https://peps.python.org/pep-0343/\n",
    "- 컨텍스트 관리자(파이썬 공식 문서): https://docs.python.org/ko/3/reference/datamodel.html#context-managers, https://docs.python.org/ko/3/library/stdtypes.html#typecontextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7246770d-9205-48af-8779-550384300163",
   "metadata": {},
   "source": [
    "### with 구문에 사용할 class를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d51a5dde-f0a0-4d3c-903e-10675cbb6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 시작\n",
      "작업 수행\n",
      "작업 종료\n"
     ]
    }
   ],
   "source": [
    "class With:\n",
    "    def __init__(self):\n",
    "        self.execute = \"작업 시작\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        print(self.execute)\n",
    "        return self\n",
    "\n",
    "    def printText(self):\n",
    "        print(\"작업 수행\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        print('작업 종료')\n",
    "\n",
    "#객체 생성 및 with 구문 실행\n",
    "withclass = With()\n",
    "\n",
    "with withclass as wc:\n",
    "    wc.printText()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f1f18-6347-4fd8-9729-3d3caa1e0050",
   "metadata": {},
   "source": [
    "#### 예외 발생 시\n",
    "- 작업 수행 시 예외가 발생하더라도 \\_\\_exit\\_\\_가 실행된다 -> 예외가 발생하더라도 파일을 닫아 리소스를 제거할 수 있다 -> 코드 안정성 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd8e109-76dc-4718-9ab5-a09a14d2ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 시작\n",
      "작업 종료\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "작업 불량",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m withclass \u001b[38;5;241m=\u001b[39m With()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m withclass \u001b[38;5;28;01mas\u001b[39;00m wc:\n\u001b[1;32m---> 19\u001b[0m     wc\u001b[38;5;241m.\u001b[39mprintText()\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mWith.printText\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprintText\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m작업 불량\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: 작업 불량"
     ]
    }
   ],
   "source": [
    "class With:\n",
    "    def __init__(self):\n",
    "        self.execute = \"작업 시작\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        print(self.execute)\n",
    "        return self\n",
    "\n",
    "    def printText(self):\n",
    "        raise Exception('작업 불량')\n",
    "\n",
    "    def __exit__(self, exc_t, exc_v, exc_tb):\n",
    "        print('작업 종료')\n",
    "\n",
    "#객체 생성 및 with 구문 실행\n",
    "withclass = With()\n",
    "\n",
    "with withclass as wc:\n",
    "    wc.printText()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a980e1-b22e-43fc-b9f3-5d0a6c67d7a5",
   "metadata": {},
   "source": [
    "## tensorflow 공식 문서 내 with가 사용된 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea81c623-81d1-44cc-b18e-3debec923456",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62dbc2-7dc3-4a6f-a21b-0f00fabb375c",
   "metadata": {},
   "source": [
    "# 3. Multi-variable Linear Regression\n",
    "- 다중 선형 회귀라고도 부르며, 하나의 종속 변수 $y$에 대해 여러 독립 변수 $x_i$를 가지는 모델로 식으로는 $H(x)=WX+b$로 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4859c11-9258-4ec9-b339-c63d49bf5c2b",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce5e57-f7f4-448c-a131-37fdf3a500d5",
   "metadata": {},
   "source": [
    "### Cost function in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d49c01-5ef8-4f4d-adee-e12e3a08d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c / len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    # np.linspace(-3, 5, num=15) -> -3 ~ 5사이에 15개의 숫자를 채워넣음\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4444428-c099-48c7-9856-37624fbb90b7",
   "metadata": {},
   "source": [
    "numpy.linspace\n",
    "- **Docstring**: Return evenly spaced numbers over a specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f86a5b2-aa11-4628-b03a-7d74a140b334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.        , -2.42857143, -1.85714286, -1.28571429, -0.71428571,\n",
       "       -0.14285714,  0.42857143,  1.        ,  1.57142857,  2.14285714,\n",
       "        2.71428571,  3.28571429,  3.85714286,  4.42857143,  5.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(-3, 5, num=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd9b1af-ef97-4ab1-b4ef-174b122f3557",
   "metadata": {},
   "source": [
    "### Cost function in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda8f6bd-de52-4d08-b927-01ad3002a957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "  hypothesis = X * W\n",
    "  return tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "W_values = np.linspace(-3, 5, num=15)\n",
    "cost_values = []\n",
    "\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdec4c0-ec3b-49a9-924b-86659c3a5e97",
   "metadata": {},
   "source": [
    "### W에 따른 Cost(W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8711421d-3173-4760-9e8c-d9bec5a992a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.plot(W_values, cost_values, \"b\")\n",
    "plt.ylabel('Cost(W)')\n",
    "plt.xlabel('W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d4182-49d3-405d-a05e-aa1bb122533b",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm in multi-variable linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b6132-db61-4749-8ab5-b709d72ad2b8",
   "metadata": {},
   "source": [
    "다음의 비용 함수에서, $m$을 $2m$으로 바꾼다하더라도 cost 계산 및 최소화 과정에 영향이 없으며 제곱을 미분할 때 공식이 단순화되는 효과가 있다. 따라서 *Formal definition*에서 $cost(W)$를 정의할 때 $m$ 대신 $2m$을 대입한 공식을 사용한다.\n",
    "\\begin{align} cost(W,b) = {1 \\over m}\\sum_{i=1}^m (Wx_i-y_i)^2 \\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727c6e2-7de4-4f2b-807e-507a2fe3df01",
   "metadata": {},
   "source": [
    "## Formal definition\n",
    "\\begin{equation}\n",
    "cost(W,b) = {1 \\over 2m}\\sum_{i=1}^m (Wx_i-y_i)^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "W := W - \\alpha{\\partial \\over \\partial W}cost(W) \n",
    "\\end{equation}\n",
    "$W$ = $W$ - 변화량<br>\n",
    "변화량 = 현 $W$에서의 비용곡선 기울기 $*$ $\\alpha$<br>\n",
    "$\\alpha$ = *learning late*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228a7de-3701-4a90-833a-c54fb7e2c92b",
   "metadata": {},
   "source": [
    "## Formal definition (continue)\n",
    "\\begin{equation}\n",
    "W := W - \\alpha{\\partial \\over \\partial W}{1 \\over 2m}\\sum_{i=1}^m (Wx_i-y_i)^2 \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "W := W - \\alpha{1 \\over 2m}\\sum_{i=1}^m 2(Wx_i-y_i)x_i\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "W := W - \\alpha{1 \\over m}\\sum_{i=1}^m (Wx_i-y_i)x_i\n",
    "\\end{equation}\n",
    "$W$ = $W$ - 변화량<br>\n",
    "변화량 = 현 $W$에서의 비용곡선 기울기 $*$ $\\alpha$<br>\n",
    "$\\alpha$ = *learning late*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3c229-46b6-4928-8233-72455d023ff9",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm\n",
    "\\begin{equation}\n",
    "W := W - \\alpha{1 \\over m}\\sum_{i=1}^m (Wx_i-y_i)x_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00907fe-2d54-44b5-85ad-43486ec95ffb",
   "metadata": {},
   "source": [
    "- **:=** (바다코끼리 연산자): 할당(binding)과 동시에 return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39905f2f-1070-4f68-b35e-fd9fdb6a1c72",
   "metadata": {},
   "source": [
    "## Gradient descent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afadcd4-985f-4110-9481-874289d08fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 고정\n",
    "# tf.set_random_seed(0)  # for reproducibility\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe19d93-2980-4aec-8e59-ffa20fb00bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 369568.2812 | -267.280273\n",
      "   10 | 142098.7500 | -165.355255\n",
      "   20 | 54636.8750 | -102.153580\n",
      "   30 | 21007.8379 | -62.963478\n",
      "   40 |  8077.4995 | -38.662479\n",
      "   50 |  3105.7939 | -23.593918\n",
      "   60 |  1194.1759 | -14.250199\n",
      "   70 |   459.1598 |  -8.456345\n",
      "   80 |   176.5466 |  -4.863691\n",
      "   90 |    67.8821 |  -2.635958\n",
      "  100 |    26.1006 |  -1.254585\n",
      "  110 |    10.0357 |  -0.398023\n",
      "  120 |     3.8587 |   0.133113\n",
      "  130 |     1.4837 |   0.462461\n",
      "  140 |     0.5705 |   0.666683\n",
      "  150 |     0.2193 |   0.793317\n",
      "  160 |     0.0843 |   0.871840\n",
      "  170 |     0.0324 |   0.920530\n",
      "  180 |     0.0125 |   0.950722\n",
      "  190 |     0.0048 |   0.969444\n",
      "  200 |     0.0018 |   0.981053\n",
      "  210 |     0.0007 |   0.988251\n",
      "  220 |     0.0003 |   0.992715\n",
      "  230 |     0.0001 |   0.995483\n",
      "  240 |     0.0000 |   0.997199\n",
      "  250 |     0.0000 |   0.998263\n",
      "  260 |     0.0000 |   0.998923\n",
      "  270 |     0.0000 |   0.999332\n",
      "  280 |     0.0000 |   0.999586\n",
      "  290 |     0.0000 |   0.999743\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])\n",
    "\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1], -100., 100.))\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis = W * X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(\n",
    "            step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d088e2-2755-41df-b949-6fc0eff003cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |    74.6667 |   4.813334\n",
      "   10 |    28.7093 |   3.364572\n",
      "   20 |    11.0387 |   2.466224\n",
      "   30 |     4.2444 |   1.909177\n",
      "   40 |     1.6320 |   1.563762\n",
      "   50 |     0.6275 |   1.349578\n",
      "   60 |     0.2413 |   1.216766\n",
      "   70 |     0.0928 |   1.134412\n",
      "   80 |     0.0357 |   1.083346\n",
      "   90 |     0.0137 |   1.051681\n",
      "  100 |     0.0053 |   1.032047\n",
      "  110 |     0.0020 |   1.019871\n",
      "  120 |     0.0008 |   1.012322\n",
      "  130 |     0.0003 |   1.007641\n",
      "  140 |     0.0001 |   1.004738\n",
      "  150 |     0.0000 |   1.002938\n",
      "  160 |     0.0000 |   1.001822\n",
      "  170 |     0.0000 |   1.001130\n",
      "  180 |     0.0000 |   1.000700\n",
      "  190 |     0.0000 |   1.000434\n",
      "  200 |     0.0000 |   1.000269\n",
      "  210 |     0.0000 |   1.000167\n",
      "  220 |     0.0000 |   1.000103\n",
      "  230 |     0.0000 |   1.000064\n",
      "  240 |     0.0000 |   1.000040\n",
      "  250 |     0.0000 |   1.000025\n",
      "  260 |     0.0000 |   1.000015\n",
      "  270 |     0.0000 |   1.000009\n",
      "  280 |     0.0000 |   1.000006\n",
      "  290 |     0.0000 |   1.000004\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])\n",
    "\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable([5.])\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis = W * X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(\n",
    "            step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85152209-2e2f-4d40-82a7-48b316475953",
   "metadata": {},
   "source": [
    "# Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8aa87-ae48-4524-9719-8d67c73ed91f",
   "metadata": {},
   "source": [
    "## dot product(=scalar product, 내적)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b5c98-e78c-4795-b288-25d08de28622",
   "metadata": {},
   "source": [
    "# Multi-feature regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747b061-c62d-4556-82b0-ef847c5fb763",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\\begin{equation}\n",
    "H(x) = wx+b\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "H(x_1, ..., x_n) = w_1x_1 + ... + w_nx_n + b\n",
    "\\end{equation}\n",
    "- 지금까지 다룬 Multi-variable과 다른 점은, $W$가 하나의 값을 가지지 않고 $n$개의 $w_i$를 가진다는 점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11c3c8-7807-4537-9ffc-6ef8819df574",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc290b-76ef-4c1c-82e9-026402044518",
   "metadata": {},
   "source": [
    "### 2 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "800f91cc-b9b7-45e2-abcd-51b6e953671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 966.489624 |    -6.3849 |    -9.6386 |  -1.997182\n",
      "   50 | 290.719482 |    -2.5520 |    -6.0406 |   0.083186\n",
      "  100 |  97.416336 |    -0.8520 |    -3.7830 |   1.185548\n",
      "  150 |  36.060600 |    -0.1155 |    -2.3524 |   1.777458\n",
      "  200 |  14.622516 |     0.1927 |    -1.4381 |   2.096439\n",
      "  250 |   6.551729 |     0.3151 |    -0.8494 |   2.265759\n",
      "  300 |   3.350713 |     0.3601 |    -0.4675 |   2.350938\n",
      "  350 |   2.032307 |     0.3750 |    -0.2176 |   2.387711\n",
      "  400 |   1.469417 |     0.3797 |    -0.0524 |   2.396086\n",
      "  450 |   1.216593 |     0.3823 |     0.0583 |   2.387553\n",
      "  500 |   1.092846 |     0.3854 |     0.1338 |   2.368865\n",
      "  550 |   1.023482 |     0.3899 |     0.1865 |   2.344070\n",
      "  600 |   0.977404 |     0.3956 |     0.2244 |   2.315640\n",
      "  650 |   0.941599 |     0.4023 |     0.2527 |   2.285101\n",
      "  700 |   0.910600 |     0.4097 |     0.2748 |   2.253407\n",
      "  750 |   0.882098 |     0.4175 |     0.2927 |   2.221159\n",
      "  800 |   0.855110 |     0.4255 |     0.3080 |   2.188733\n",
      "  850 |   0.829210 |     0.4337 |     0.3214 |   2.156371\n",
      "  900 |   0.804204 |     0.4420 |     0.3335 |   2.124223\n",
      "  950 |   0.779998 |     0.4502 |     0.3448 |   2.092385\n",
      " 1000 |   0.756540 |     0.4584 |     0.3556 |   2.060913\n"
     ]
    }
   ],
   "source": [
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform([1], -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform([1], -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform([1], -10.0, 10.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d3e5c-1c0b-4963-b133-6449db2a7df5",
   "metadata": {},
   "source": [
    "### with Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bc765d3b-a5b8-467c-87b1-b7e4f1b310f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   5.769839 |     0.3661 |    -0.2659 |   0.788406\n",
      "   50 |   2.157725 |     0.5734 |     0.0667 |   0.926181\n",
      "  100 |   0.946475 |     0.6632 |     0.2795 |   0.996021\n",
      "  150 |   0.501840 |     0.7014 |     0.4170 |   1.029126\n",
      "  200 |   0.326270 |     0.7175 |     0.5069 |   1.041886\n",
      "  250 |   0.252195 |     0.7247 |     0.5665 |   1.043127\n",
      "  300 |   0.218308 |     0.7286 |     0.6066 |   1.037728\n",
      "  350 |   0.200848 |     0.7315 |     0.6342 |   1.028455\n",
      "  400 |   0.190266 |     0.7344 |     0.6537 |   1.016918\n",
      "  450 |   0.182646 |     0.7374 |     0.6680 |   1.004070\n",
      "  500 |   0.176359 |     0.7406 |     0.6789 |   0.990489\n",
      "  550 |   0.170724 |     0.7440 |     0.6875 |   0.976528\n",
      "  600 |   0.165453 |     0.7475 |     0.6947 |   0.962407\n",
      "  650 |   0.160421 |     0.7510 |     0.7009 |   0.948262\n",
      "  700 |   0.155575 |     0.7546 |     0.7065 |   0.934180\n",
      "  750 |   0.150889 |     0.7582 |     0.7116 |   0.920212\n",
      "  800 |   0.146349 |     0.7618 |     0.7164 |   0.906394\n",
      "  850 |   0.141949 |     0.7654 |     0.7210 |   0.892743\n",
      "  900 |   0.137682 |     0.7689 |     0.7254 |   0.879273\n",
      "  950 |   0.133543 |     0.7724 |     0.7297 |   0.865991\n",
      " 1000 |   0.129530 |     0.7758 |     0.7338 |   0.852898\n"
     ]
    }
   ],
   "source": [
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1, 2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de727d-712d-4ada-a8cc-da2fea120b48",
   "metadata": {},
   "source": [
    "## Hypothesis without b\n",
    "\\begin{equation}\n",
    "H(x_1, ..., x_n) = w_1x_1 + w_2x_2 + w_3x_3 + b\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " = b + w_1x_1 + w_2x_2 + w_3x_3\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "b & x_1 & x_2 & x_3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127cedd-8ef6-48c1-9e87-51ea3e0478df",
   "metadata": {},
   "source": [
    "- tf.train.<span style=\"color:skyblue\">**GradientDescentOptimizer**</span>(learning_rate) -> 텐서플로우 2.0으로 업데이트되면서 사라짐\n",
    "- tf.keras.<span style=\"color:skyblue\">**optimizers.SGD**</span>(learning_rate)로 대체ts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eff8db-92bd-4501-b5fe-e73223ac3500",
   "metadata": {},
   "source": [
    "- tf.keras.<span style=\"color:skyblue\">**optimizers.SGD**</span>(learning_rate)<br>Gradient descent (with momentum) optimizer. \r\n",
    "Update rule for parameter `w` with gradient `g` when `momentum` is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ebd9729-f3db-4fe2-afd7-8a1a2647cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  15.424617 |    -0.5626 |     0.3981 |    -0.2507\n",
      "   50 |   0.000958 |    -0.0140 |     1.0048 |     0.9904\n",
      "  100 |   0.000015 |    -0.0091 |     1.0024 |     1.0027\n",
      "  150 |   0.000011 |    -0.0078 |     1.0020 |     1.0024\n",
      "  200 |   0.000008 |    -0.0067 |     1.0017 |     1.0021\n",
      "  250 |   0.000006 |    -0.0057 |     1.0015 |     1.0018\n",
      "  300 |   0.000004 |    -0.0049 |     1.0013 |     1.0015\n",
      "  350 |   0.000003 |    -0.0042 |     1.0011 |     1.0013\n",
      "  400 |   0.000002 |    -0.0036 |     1.0009 |     1.0011\n",
      "  450 |   0.000002 |    -0.0031 |     1.0008 |     1.0010\n",
      "  500 |   0.000001 |    -0.0027 |     1.0007 |     1.0008\n",
      "  550 |   0.000001 |    -0.0023 |     1.0006 |     1.0007\n",
      "  600 |   0.000001 |    -0.0020 |     1.0005 |     1.0006\n",
      "  650 |   0.000001 |    -0.0017 |     1.0004 |     1.0005\n",
      "  700 |   0.000000 |    -0.0014 |     1.0004 |     1.0005\n",
      "  750 |   0.000000 |    -0.0012 |     1.0003 |     1.0004\n",
      "  800 |   0.000000 |    -0.0011 |     1.0003 |     1.0003\n",
      "  850 |   0.000000 |    -0.0009 |     1.0002 |     1.0003\n",
      "  900 |   0.000000 |    -0.0008 |     1.0002 |     1.0002\n",
      "  950 |   0.000000 |    -0.0007 |     1.0002 |     1.0002\n",
      " 1000 |   0.000000 |    -0.0006 |     1.0002 |     1.0002\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], # bias(b)\n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1, 3], -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652bf76d-4bf1-4c15-8568-1c93442ffd2a",
   "metadata": {},
   "source": [
    "## Predicting exam score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ba31da2-f38b-40be-bbc0-d56b09c86118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 | 75475.8672\n",
      "  100 |    74.2499\n",
      "  200 |    64.6160\n",
      "  300 |    64.2665\n",
      "  400 |    63.9203\n",
      "  500 |    63.5757\n",
      "  600 |    63.2334\n",
      "  700 |    62.8924\n",
      "  800 |    62.5536\n",
      "  900 |    62.2165\n",
      " 1000 |    61.8813\n",
      " 1100 |    61.5478\n",
      " 1200 |    61.2163\n",
      " 1300 |    60.8863\n",
      " 1400 |    60.5584\n",
      " 1500 |    60.2319\n",
      " 1600 |    59.9077\n",
      " 1700 |    59.5848\n",
      " 1800 |    59.2638\n",
      " 1900 |    58.9444\n",
      " 2000 |    58.6269\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d051a0-1131-45d7-b7d1-711207747afb",
   "metadata": {},
   "source": [
    "quiz 1: 50점\n",
    "quiz 2: 40점\n",
    "mid 1: 60점\n",
    "최종 점수는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "809408f9-5bf9-4885-b8ff-e8c300c18ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[82.57225]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([[50.,40.,60.]]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a497a9-1876-4558-afbf-53f405a6c774",
   "metadata": {},
   "source": [
    "# Logistic Regression (Classification)<br>- Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a778a-6b30-4225-95ee-bd40d72aa41d",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀: 독립 변수들을 통해 True/False와 같은 이진 분류 확률을 추정하는 함수로, 결과가 확률이기 때문에 종속 변수는 0과 1 사이에 있다.\n",
    "- 결과를 확률로 나타낸 후, 시그모이드 함수를 활용하여 확률값을 0과 1의 값으로 변환한다.\n",
    "- 시그모이드 함수 형태는 다음과 같으며, 텐서플로우에선 tensorflow.sigmoid로 계산한다.\n",
    "\\begin{align}\n",
    "sigmoid(x) & = {1 \\over 1+e^{-x}}\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "h(x) & = {1 \\over 1+e^{-(WX+b)}}\n",
    "\\end{align}\n",
    "- 시그모이드 함수를 통해 예측값이 0.5보다 크면 1 (주로 True)을 반환하고 0.5보다 작으면 0 (주로 False)을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7cf78203-16b9-4bab-946a-917fe7443dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f3879-8306-402c-a839-0332af6ea5a4",
   "metadata": {},
   "source": [
    "## Goal\n",
    "- 보라색과 노란색 data를 학습시킨 뒤 test data (빨간색)에 대해 추론한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ecc56509-51dd-4480-a2fa-ba69ed792ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = [[1., 2.],\n",
    "          [2., 3.],\n",
    "          [3., 1.],\n",
    "          [4., 3.],\n",
    "          [5., 3.],\n",
    "          [6., 2.]]\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]\n",
    "\n",
    "x_test = [[5.,2.]]\n",
    "y_test = [[1.]]\n",
    "\n",
    "\n",
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "\n",
    "colors = [int(y[0] % 3) for y in y_train]\n",
    "plt.scatter(x1,x2, c=colors , marker='^')\n",
    "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66414860-0ad7-4b07-b3f3-4694c02e44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e786c-594d-4c93-89de-50a9b18e9895",
   "metadata": {},
   "source": [
    "## W, b 초기값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d65897d-f519-45f0-a344-ad3c591dfbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463fa5b-af69-4364-8f28-ebe471a9b21c",
   "metadata": {},
   "source": [
    "## logistic regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d607a814-f485-49ce-abda-08dfd9d7f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8e6f1-9946-4d17-ab2c-259d25684daa",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "cost(h(x),y) = −log(h(x))\\qquad if \\quad y=1\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "cost(h(x),y) = −log(1-h(x))\\qquad if \\quad y=0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae5539-a1c2-46f4-be67-0ce28ec62763",
   "metadata": {},
   "source": [
    "- 위 두 수식을 하나의 수식으로 재표현\n",
    "\\begin{align}\n",
    "cost(h(x),y) = -ylog(h(x))−(1-y)log(1-h(x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71507b3-74a3-4b05-81f2-5e7a7fee8f5a",
   "metadata": {},
   "source": [
    "## Loss function of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3a21cfad-2aa5-47b8-b4f2-96a717164539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3145b67-d236-4eda-af90-c65bc04b8fc2",
   "metadata": {},
   "source": [
    "## Function to calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "34163d37-12de-4758-bdc5-5d14b1ae57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a09ef-834f-4302-86f3-9e66749ee848",
   "metadata": {},
   "source": [
    "## Function to calculate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3dcbccde-61ca-4efc-a7e7-b45ade8adcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1caec5-e794-4ee2-a60e-d72429940106",
   "metadata": {},
   "source": [
    "## Train\n",
    "- Test data -> 1로 추론 (정답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c6f0127f-a6dc-493e-860a-a847fe1fe43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6874\n",
      "Iter: 100, Loss: 0.5776\n",
      "Iter: 200, Loss: 0.5349\n",
      "Iter: 300, Loss: 0.5054\n",
      "Iter: 400, Loss: 0.4838\n",
      "Iter: 500, Loss: 0.4671\n",
      "Iter: 600, Loss: 0.4535\n",
      "Iter: 700, Loss: 0.4420\n",
      "Iter: 800, Loss: 0.4319\n",
      "Iter: 900, Loss: 0.4228\n",
      "Iter: 1000, Loss: 0.4144\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04025aa-72a7-4f37-9522-fb8f469cc5ae",
   "metadata": {},
   "source": [
    "# Logistic Regression을 통한 diabetes(당뇨병) 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7038fb6a-b12b-4f14-bdff-2eb9211579b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "[[-0.294118   0.487437   0.180328  ... -0.53117   -0.0333333  0.       ]\n",
      " [-0.882353  -0.145729   0.0819672 ... -0.766866  -0.666667   1.       ]\n",
      " [-0.0588235  0.839196   0.0491803 ... -0.492741  -0.633333   0.       ]\n",
      " ...\n",
      " [-0.411765   0.21608    0.180328  ... -0.857387  -0.7        1.       ]\n",
      " [-0.882353   0.266332  -0.0163934 ... -0.768574  -0.133333   0.       ]\n",
      " [-0.882353  -0.0653266  0.147541  ... -0.797609  -0.933333   1.       ]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_train = xy[:, 0:-1]\n",
    "y_train = xy[:, [-1]]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b4295-1c93-4ac7-9143-36102bcd6d97",
   "metadata": {},
   "source": [
    "tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "beff0d6e-453c-409b-aac7-070fbcfff061",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "W = tf.Variable(tf.random.normal((8, 1)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((1,)), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00342380-0f15-4c61-97bb-b8c8e540c505",
   "metadata": {},
   "source": [
    "## logistic regression function (위와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4f0d8-03a3-43cc-ab50-12aeb3b7d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6fdf7ad1-fea9-421c-a9df-6d89d473d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss, accuracy, gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3adfaa4-e932-41ed-9c5d-be2fdb1551ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af070d55-bde0-470b-aae8-c72ba66d80eb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5bd1a3e3-7d0c-4c7c-beda-d4acc2818570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6227\n",
      "Iter: 100, Loss: 0.6170\n",
      "Iter: 200, Loss: 0.6116\n",
      "Iter: 300, Loss: 0.6064\n",
      "Iter: 400, Loss: 0.6015\n",
      "Iter: 500, Loss: 0.5968\n",
      "Iter: 600, Loss: 0.5924\n",
      "Iter: 700, Loss: 0.5881\n",
      "Iter: 800, Loss: 0.5840\n",
      "Iter: 900, Loss: 0.5802\n",
      "Iter: 1000, Loss: 0.5765\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ada7e-bd4c-40cf-8d9e-761088e169cf",
   "metadata": {},
   "source": [
    "# 더 생각해볼 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1afee9-f82f-4282-a406-bf8dff24d8b8",
   "metadata": {},
   "source": [
    "## tensorflow의 데이터 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "925658d7-335a-4f48-879c-cff95b506d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2683318b-434d-4c7d-ac18-1e37cce18373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ = tf.constant([1,2,3,4])\n",
    "np_ = np.array([1,2,3,4])\n",
    "torch_ = torch.Tensor([1,2,3,4])\n",
    "list_ = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd091aa2-47de-4b93-a74e-df3c69e1ddfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ + list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b1e0cc-4f2c-4848-8bbd-2709cb493c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(list_,list_):\n",
    "    print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f05265a-c1eb-4451-8ed8-adc92341a5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 4, 6, 8])>,\n",
       " array([ 1,  4,  9, 16]),\n",
       " tensor([2., 4., 6., 8.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ + tf_, np_ * np_, torch_ + torch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ae074-1d52-4bd3-9d48-503a1bdd1699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af4921-a069-44c3-98c3-887b8cd77e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
