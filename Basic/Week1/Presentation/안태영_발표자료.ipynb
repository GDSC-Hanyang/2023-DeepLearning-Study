{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 1: Machine Learning**"
      ],
      "metadata": {
        "id": "3gYo_ndZ579x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Machine Learning** - 기계학습  \n",
        "&rarr; 컴퓨터가 배워서 학습하는 것\n",
        "\n",
        "> **Supervised Learning** -  지도 학습  \n",
        "    - Training Data Set을 이용함\n",
        "    - Label이 있어서 T or F 판단가능  \n",
        "    ex)\n",
        ">   >- **Regression** &rarr; final exam score based on time spent  \n",
        ">   >- **Binary Classification** &rarr;  pass or fail  \n",
        ">   >- **Multi Label- Classification** &rarr; Grades (A, B, C, D, or F)  \n",
        "\n",
        "> **Unsupervised Learning** - 비지도 학습  \n",
        "     - Un- Label data를 이용하여 직접 스스로 학습  \n",
        "     ex) Grouping, Clustering\n",
        "\n",
        "> **Trainging Data Set**\n",
        "- Train을 시키기 위한 Data 집합\n",
        "- (X, Y)로 나와 있으며 X를 **feature**, Y를 **value**라고 한다\n",
        "   "
      ],
      "metadata": {
        "id": "hopSK9Fn6Fn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 2: Simple Linear Regression**"
      ],
      "metadata": {
        "id": "9KNdxz6yh4fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Regression**  \n",
        "- 평균에 되돌아가려는 통계학적 용어\n",
        "\n",
        "> **Linear Regression** - 선형회귀  <br><br>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1920px-Linear_regression.svg.png\" width=\"550\" height=\"300\">  <br><br>\n",
        "- $Y = ax + b$ 형태  \n",
        "- 데이터가 여러 개 있을 때, 가장 근접하게 나타 낼 수 있는 1차 함수를 나타내는 것  \n",
        "- 가장 가깝게 근사하는 과정을 '**fiiting** 한다'라고 한다.\n",
        "\n",
        "> **Hypothesis**  - 가설  \n",
        "- $H(x) = Wx + b$  \n",
        "- $W$를 wieght(가중치), $b$를 bias(편향)이라 정의한다  \n",
        "\n",
        "> **Cost Function** - 비용 함수<br><br>\n",
        "<img src=\"https://leechanhyuk.github.io/assets/image/cost_function/mae_graph.png\" width=\"550\" height=\"300\"><br><br>\n",
        "- $Error = H(x) - Y$  \n",
        "- Error가 가장 작은 값이 fitting이 잘 되었다고 한다  \n",
        "- 한계점 : Error 중에는 + 와 - 가 있어서 제대로 정의할 수 없다\n",
        "- $$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (H({ x }^{ i })-y^{ i } })^{ 2 } }  $$  \n",
        "- 따라서 Error 제곱의 평균을 Cost로 다시 정의하여 사용한다  \n",
        "- Linear Regression는 Error를 **Minimize** 하는 직선을 찾는 과정이다"
      ],
      "metadata": {
        "id": "2u43GWBOC-3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LAB 02: Simple Linear Regression 를 TensorFlow 로 구현하기**"
      ],
      "metadata": {
        "id": "9ogcRW7FgQNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **tf.reduce_mean( )**  \n",
        "&rarr; 평균을 tensor object로 반환  \n",
        "- **tf.square( )**  \n",
        "&rarr; 제곱근 값을 tensor object로 반환"
      ],
      "metadata": {
        "id": "3HZauNh8iNWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = [1., 2., 3., 4.]\n",
        "tf.reduce_mean(v) # 2.\n",
        "tf.square(3) # 9"
      ],
      "metadata": {
        "id": "K71Fr6gwiMe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **tf.Variable( )**  \n",
        "&rarr; 값이 변경 가능한 변수를 정의(공간만 할당)   \n",
        "&rarr; 중간 계산 과정에서 재할당이 계속 이루어진다  \n",
        "&rarr; **.assign( )**을 통해 값 할당 가능  \n",
        "cf) **tf.Constant( )** &rarr; 값 변경 불가  \n",
        "\n",
        "- **tf.placeholder(** dtype, shape=Nonem name=None **)**    \n",
        "&rarr; 선언과 동시에 초기화 시키는 것이 아니고 선언한 후 값을 전달한다  \n",
        "&rarr; placeholder에 Tensor를 맵핑 시키는 것으로 사용  \n",
        "&rarr; tensorflow1에서만 사용 가능하다"
      ],
      "metadata": {
        "id": "dIfC2JhZlIRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = tf.Variable([1,2])\n",
        "print(v)                #[1, 2]\n",
        "v.assign([3,4])\n",
        "print(v)                #[3, 4]\n",
        "\n",
        "#p_holder = tf.placeholder(tf.float32)"
      ],
      "metadata": {
        "id": "6Q0X-mSMopjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Tensorflow**  \n",
        "&rarr; 강의에서 Tensorflow1을 이용한 코드를 강의하므로 Tensorflow2로 고쳐서 진행하였다.\n",
        "- **Numpy**  \n",
        "&rarr; Tensorflow와 데이터 가공을 위한 numpy 모듈을 import 한다."
      ],
      "metadata": {
        "id": "n1bSoHcbifLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgBCKUv7hxt4"
      },
      "outputs": [],
      "source": [
        "#Tenosorflow1을 사용하는 코드\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior()\n",
        "\n",
        "#Tensorflow2를 사용하는 코드\n",
        "import tensorflow as tf\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Full Code of Lecture**"
      ],
      "metadata": {
        "id": "ZpN-ZXcZW1wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.executing_eagerly()\n",
        "\n",
        "x_data = np.array(list(range(1,6)))\n",
        "y_data = np.array(list(range(1,6)))\n",
        "plt.plot(x_data, y_data, 'bo')\n",
        "\n",
        "W = tf.Variable(2.9)    #Weight\n",
        "b = tf.Variable(0.5)    #Bias\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(101):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = W * x_data + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "    W_grad, b_grad = tape.gradient(cost, [W,b])\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    if i % 10 == 0:\n",
        "        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
        "        plt.plot(x_data, W.numpy() * x_data + b.numpy())\n",
        "\n",
        "plt.axis([0, max(x_data) + 1, 0, max(y_data) + 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zIpcSBSwA26G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Lecture 03: Linear Regression, How to minimize cost**"
      ],
      "metadata": {
        "id": "5MB4EQ9G2wmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Hypothesis & Cost**  \n",
        "- $H(x) = Wx + b$  \n",
        "- $ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }_{ i }-y_{ i } })^{ 2 } }$  \n",
        "\n",
        "> **Simplified Hypothesis & Cost**  \n",
        "- $H(x) = Wx$\n",
        "- $ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }_{ i }-y_{ i } })^{ 2 } }  $  \n",
        "\n",
        "> **How to minimize the cost?**  \n",
        "&rarr; cost가 가장 작게 되는 $W$를 찾는 것이다  \n",
        "\n",
        "> **Gradient Descent Algorithm**<br><br>\n",
        "<img src=\"https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png\" width=\"550\" height=\"300\"><br><br>\n",
        "- Initial Value에서 $W$와 $b$값을 cost가 최소가 될 때까지 업데이트 하여 최소점(Minimum)에 도달할때까지 시행한다  \n",
        "- Cost를 최소화 하는 Algorithm  \n",
        "\n",
        "> **Formal Definition**  \n",
        "- $ cost(W, b)=\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (H({ x }_{ i })-y_{ i } })^{ 2 } }  $  <br><br>\n",
        "&rarr; $m$으로 나누나 $2m$으로 나누나 cost에 영향을 주지 않는다  <br><br>\n",
        "- $H({x}^{i})$를 simplified 된 $W{x}^{i}$로 사용한다<br><br>\n",
        "- $cost(W, b)=\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (H({ x }_{ i })-y_{ i } })^{ 2 } } $  <br><br>\n",
        "- $W = W - \\alpha\\frac{\\partial}{\\partial W}(\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (H({ x }_{ i })-y_{ i } })^{ 2 } } )$<br><br>\n",
        "- $W = W - \\alpha\\frac { 1 }{ 2m }\\sum _{i=1}^{m}{ 2{ (H({ x }_{ i })-y_{ i } }) }x_{i}$ <br><br>\n",
        "- $W = W - \\alpha\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (H({ x }_{ i })-y_{ i } }) }x_{i}$<br><br>\n",
        "&rarr; 항상 그 자리의 $x_{i}$ 값에 Gradient를 곱해서 그 다음 $W$로 이동한다\n"
      ],
      "metadata": {
        "id": "A7kmaDc22wNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 03: Linear Regression and How to minimize cost 를 TensorFlow 로 구현하기**"
      ],
      "metadata": {
        "id": "8sOgWm9r23nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Cost Function in Pure Python**  \n",
        "&rarr; numpy 모듈을 활용해서 함수를 직접 만든다"
      ],
      "metadata": {
        "id": "tEquG0HABB6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3])\n",
        "Y = np.array([1, 2, 3])\n",
        "\n",
        "#Cost Function\n",
        "def cost_func(W, X, Y):\n",
        "    c = 0\n",
        "    for i in range(len(X)):\n",
        "        c += (W * X[i] - Y [i] ) ** 2\n",
        "    return c/ len(X)\n",
        "\n",
        "for feed_W in np.linspace(-3, 5, num=15):\n",
        "    curr_cost  = cost_func(feed_W, X, Y)\n",
        "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
      ],
      "metadata": {
        "id": "kXZxLI-_22bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cost Function in Tensorflow**  \n"
      ],
      "metadata": {
        "id": "wDa-w4oNDaO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 제곱의 평균을 **tf.reduce_mean( )**과 **tf.square( )**을 사용한다"
      ],
      "metadata": {
        "id": "zI0dR9iePZDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 2, 3])\n",
        "Y = np.array([1, 2, 3])\n",
        "def cost_func(W, X, Y):\n",
        "    hypothesis = X * W\n",
        "    return tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "W_values = np.linspace(-3, 5, num=15)\n",
        "cost_values = []\n",
        "for feed_W in W_values:\n",
        "    curr_cost = cost_func(feed_W, X, Y)\n",
        "    cost_values.append(curr_cost)\n",
        "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
      ],
      "metadata": {
        "id": "q99YYpwvDkWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Descent**  "
      ],
      "metadata": {
        "id": "7VbVwtlPTwVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- gradient 를 **tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))** 로 표현한다"
      ],
      "metadata": {
        "id": "XXrA8-f3V51s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(0) # for reproducibility\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [1., 3., 5., 7.]\n",
        "W = tf.Variable(tf.random.normal([1], -100., 100.))\n",
        "for step in range(300):\n",
        "    hypothesis = W * X\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    alpha = 0.01\n",
        "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
        "    descent = W - tf.multiply(alpha, gradient)\n",
        "    W.assign(descent)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))"
      ],
      "metadata": {
        "id": "LlrlNN7CTvvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 04: Multi Variable Linear Regression**"
      ],
      "metadata": {
        "id": "znPtzw6TD2SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **More than 2 variables**  \n",
        "- $x_{1}, x_{2}, x_{3}, \\cdots$  \n",
        "- ex)  \n",
        "$(x_{1}, x_{2}, x_{3})$  \n",
        "$H(x_{1}, x_{2}, x_{3}) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$  \n",
        "$ cost(W, b)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (H({ x }_{ 1 }, x_{2}, x_{3})-y_{ i } })^{ 2 } }$\n",
        "\n",
        "> **Matrix**  \n",
        "$H_{n}(x) = w_{1}x_{1} + w_{2}x_{2} \\cdots w_{n}x_{n} =  \\left[\\begin{matrix} x_{1} & x_{2} & \\cdots & x_{n} \\end{matrix}\\right] \\left[\\begin{matrix}w_{1}\\\\ w_{2}\\\\ \\cdots \\\\ w_{n} \\end{matrix}\\right] $  \n",
        "$H(X) = XW$<br><br>\n",
        "&rarr; 이때 $X$는 $x$ 값들의 집합, $W$는 weight의 집합이다  \n",
        "\n",
        ">**$WX$ VS $XW$**  \n",
        "- Theory  \n",
        "&rarr; $H(x) = Wx + b$  \n",
        "&rarr; $h_{\\Theta}(x) = \\theta_{1}x+\\theta_{0}$<br><br>\n",
        "-Tensorflow  \n",
        "&rarr; $H(X) = XW$"
      ],
      "metadata": {
        "id": "0rAwmFOjD27n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 04: Multi-variable Linear Regression 를 TensorFlow 로 구현하기**"
      ],
      "metadata": {
        "id": "cH8IloQFXcsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hypothesis Multi-Variable**   \n",
        "&rarr; 앞서 사용한 list를 그대로 사용한 코드"
      ],
      "metadata": {
        "id": "LX7PPE6GXf-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data and label\n",
        "x1 = [ 73., 93., 89., 96., 73.]\n",
        "x2 = [ 80., 88., 91., 98., 66.]\n",
        "x3 = [ 75., 93., 90., 100., 70.]\n",
        "Y = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# random weights\n",
        "w1 = tf.Variable(tf.random.normal([1]))\n",
        "w2 = tf.Variable(tf.random.normal([1]))\n",
        "w3 = tf.Variable(tf.random.normal([1]))\n",
        "b  = tf.Variable(tf.random.normal([1]))\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "\n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "metadata": {
        "id": "4-r5LRsjXXfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Using Matrix**  \n",
        "&rarr; data를 numpy를 이용해 정리한다\n",
        "```\n",
        "data = np.array([\n",
        "    # X1, X2, X3, y\n",
        "    [ 73., 80., 75., 152. ],\n",
        "    [ 93., 88., 93., 185. ],\n",
        "    [ 89., 91., 90., 180. ],\n",
        "    [ 96., 98., 100., 196. ],\n",
        "    [ 73., 66., 70., 142. ]\n",
        "], dtype=np.float32)\n",
        "```\n",
        "- **$H(X) = XW$**  \n",
        "&rarr; Simplified Hypothesis 이용  \n",
        "```  \n",
        "# initialize W\n",
        "W = tf.Variable(tf.random_normal([3, 1]))\n",
        "# hypothesis, prediction function\n",
        "tf.matmul(X, W) + b\n",
        "# updates parameters (W and b)\n",
        "W.assign_sub(learning_rate * W_grad)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SR8m26HSXWk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([\n",
        "    # X1, X2, X3, y\n",
        "    [ 73., 80., 75., 152. ],\n",
        "    [ 93., 88., 93., 185. ],\n",
        "    [ 89., 91., 90., 180. ],\n",
        "    [ 96., 98., 100., 196. ],\n",
        "    [ 73., 66., 70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]"
      ],
      "metadata": {
        "id": "BNAEaNGtZyMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random.normal([3, 1]))\n",
        "b = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs+1):\n",
        "    # record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    # updates parameters (W and b)\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))\n"
      ],
      "metadata": {
        "id": "6WkQMymtZ8zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Lecture 05: Logistic Regression**"
      ],
      "metadata": {
        "id": "Qg1E0IboPdfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Classification - Binary Classification**  \n",
        "- T or F로 나타낼 수 있는 분류 (0:True, 1: False)\n",
        "- 행렬로 표현된다 [1, 0, 1, 0, 0, 0]  \n",
        "\n",
        ">**Linear VS Logistic**  \n",
        ">>**Linear**  \n",
        "- continuous  \n",
        "- measured  \n",
        "-time, weight, height  \n",
        "\n",
        ">>**Logistic**  \n",
        "- discrete  \n",
        "- counted  \n",
        "- shoes size, the number of house  \n",
        "\n",
        ">**Hypothesis representation**<br><br>\n",
        "<img src=\"https://saedsayad.com/images/LogReg_1.png\" width=\"550\" height=\"300\"><br><br>\n",
        "- 0과 1을 잘 분리할 수 있는 직선을 찾는 것이다<br><br>\n",
        "- $h_{\\theta} = g(\\theta^{T}X)$<br><br>\n",
        "- $ X (Variable)=> \\theta^{T} (Linear) => g(\\theta^{T}X) (Logistic) => 0.5> (Decision Boundary)$$\n",
        "\n",
        ">**Sigmoid Fucntion**<br><br>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png\" width=\"550\" height=\"300\"><br><br>\n",
        "$g(z) = \\frac{1}{1+e^{-z}}$  \n",
        "$z \\to 0 \\Rightarrow g(z) \\to 0$  \n",
        "$z \\to \\infty \\Rightarrow g(z) \\to 1$  \n",
        ">**Decision Boundary**  \n",
        "- $g(z)$에서  \n",
        "$W\\theta^{T}$ > 0 에선 0  \n",
        "$W\\theta^{T}$ < 1 에선 1  \n",
        "$\\theta^{T}$ 를 결정 짓는 값  \n",
        "\n",
        "> **Cost Function**<br><br>\n",
        "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/ng_cost_function_logistic.png\" width=\"550\" height=\"330\"><br><br>\n",
        "> **Optimization**  \n"
      ],
      "metadata": {
        "id": "ehSnsLyTPnh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 05 Logistic Classification(Regression) - Eager Execution**"
      ],
      "metadata": {
        "id": "tgYS0WkBb-To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Tensorflow 버전 2.X 로 확인**"
      ],
      "metadata": {
        "id": "AkNLbzMTmEBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(777)  # for reproducibility\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "bap8VOgGk1UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Data 가공**"
      ],
      "metadata": {
        "id": "M7br63fRmIcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [[1., 2.],\n",
        "          [2., 3.],\n",
        "          [3., 1.],\n",
        "          [4., 3.],\n",
        "          [5., 3.],\n",
        "          [6., 2.]]\n",
        "y_train = [[0.],\n",
        "          [0.],\n",
        "          [0.],\n",
        "          [1.],\n",
        "          [1.],\n",
        "          [1.]]\n",
        "\n",
        "x_test = [[5.,2.]]\n",
        "y_test = [[1.]]\n",
        "\n",
        "\n",
        "x1 = [x[0] for x in x_train]\n",
        "x2 = [x[1] for x in x_train]\n",
        "\n",
        "colors = [int(y[0] % 3) for y in y_train]\n",
        "plt.scatter(x1,x2, c=colors , marker='^')\n",
        "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
        "\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bLZ4SjmikvfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **tensor에 data를 담는다**  \n",
        "- **Wieght와 bias initialize**"
      ],
      "metadata": {
        "id": "UV1B5jULnNJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()\n",
        "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pb4meNmulOav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Logistic Regression**  \n",
        "&rarr; sigmoid로 정의"
      ],
      "metadata": {
        "id": "wGT_OZVQnl66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(features):\n",
        "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
        "    return hypothesis"
      ],
      "metadata": {
        "id": "5zGrtsQhnmaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Cost Function**  \n",
        "$\n",
        "\\begin{align}\n",
        "cost(h(x),y) & = −y log(h(x))−(1−y)log(1−h(x))\n",
        "\\end{align}\n",
        "$"
      ],
      "metadata": {
        "id": "qJBrDRDHlaVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(hypothesis, features, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "ytBWwZ75lbX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Accuracy**  \n",
        "&rarr; 추론한 값은 0.5를 기준(Sigmoid 그래프 참조)로 0과 1의 값을 리턴합니다.  \n",
        "&rarr; Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환합니다.  \n",
        "&rarr; 가설을 통해 실재 값과 비교한 정확도를 측정합니다"
      ],
      "metadata": {
        "id": "-fe9Yf_Il1-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "RlheAqppl8ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Gradient 계산하기**"
      ],
      "metadata": {
        "id": "ST0pJSJrpWyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
        "    return tape.gradient(loss_value, [W,b])"
      ],
      "metadata": {
        "id": "AWbFmAQVpYR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **학습을 실행합니다**  \n",
        "&rarr; 위의 Data를 Cost함수를 통해 학습시킨 후 모델을 생성합니다.   \n",
        "&rarr; 새로운 Data를 통한 검증 수행 [5,2]의 Data로 테스트 수행 (그래프상 1이 나와야 정상입니다)  \n",
        "&rarr; Epoch은 몇번 실행할 것인지 말하는 것이다"
      ],
      "metadata": {
        "id": "KHYo-YDkmNCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        grads = grad(features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
        "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "2AgbZo9aoUAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MRAYZszEfRXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}